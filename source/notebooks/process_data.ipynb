{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/20 11:25:10 WARN Utils: Your hostname, Ubuntu-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/10/20 11:25:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/20 11:25:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Extraction_Data\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datetime.now().year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = 2023\n",
    "ano_final = 2024\n",
    "for ano in range(int(ano_inicial), int(ano_final) + 1):\n",
    "    print(ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = current_year\n",
    "ano_final = current_year\n",
    "\n",
    "dif_anos = (ano_final - ano_inicial) + 1\n",
    "for i in range(dif_anos):\n",
    "    print(ano_inicial + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- autorEmenda: string (nullable = true)\n",
      " |-- codigoAmparoLegal: long (nullable = true)\n",
      " |-- descricaoEmpenho: string (nullable = true)\n",
      " |-- fonteRecurso: string (nullable = true)\n",
      " |-- idProjetoInvestimento: string (nullable = true)\n",
      " |-- informacoesComplementares: string (nullable = true)\n",
      " |-- localEntrega: string (nullable = true)\n",
      " |-- naturezaDespesa: string (nullable = true)\n",
      " |-- nomeEsferaOrcamentaria: string (nullable = true)\n",
      " |-- nomeFavorecido: string (nullable = true)\n",
      " |-- nomeTipoEmpenho: string (nullable = true)\n",
      " |-- nrPtres: string (nullable = true)\n",
      " |-- numeroNotaEmpenhoGerada: string (nullable = true)\n",
      " |-- numeroProcesso: string (nullable = true)\n",
      " |-- pagina: long (nullable = true)\n",
      " |-- planoInterno: string (nullable = true)\n",
      " |-- planoOrcamentario: string (nullable = true)\n",
      " |-- resultadoPrimario: string (nullable = true)\n",
      " |-- tamanhoDaPagina: long (nullable = true)\n",
      " |-- tipoCredito: string (nullable = true)\n",
      " |-- ugEmitente: string (nullable = true)\n",
      " |-- ugResponsavel: long (nullable = true)\n",
      " |-- unidadeOrcamentaria: string (nullable = true)\n",
      " |-- valorEmpenho: double (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring\n",
    "\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "process_all = True\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/' + str(ano_inicial)+ '.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "\n",
    "#print(df.show())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cep: string (nullable = true)\n",
      " |-- dataCadastro: date (nullable = true)\n",
      " |-- dataFinalEfetiva: string (nullable = true)\n",
      " |-- dataFinalPrevista: string (nullable = true)\n",
      " |-- dataInicialEfetiva: string (nullable = true)\n",
      " |-- dataInicialPrevista: string (nullable = true)\n",
      " |-- dataSituacao: string (nullable = true)\n",
      " |-- descPlanoNacionalPoliticaVinculado: string (nullable = true)\n",
      " |-- descPopulacaoBeneficiada: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      " |-- eixos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |-- endereco: string (nullable = true)\n",
      " |-- especie: string (nullable = true)\n",
      " |-- executores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- fontesDeRecurso: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- origem: string (nullable = true)\n",
      " |    |    |-- valorInvestimentoPrevisto: double (nullable = true)\n",
      " |-- funcaoSocial: string (nullable = true)\n",
      " |-- geometrias: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- cepAreaExecutora: string (nullable = true)\n",
      " |    |    |-- dataCriacao: string (nullable = true)\n",
      " |    |    |-- dataMetadado: string (nullable = true)\n",
      " |    |    |-- datum: string (nullable = true)\n",
      " |    |    |-- enderecoAreaExecutora: string (nullable = true)\n",
      " |    |    |-- geometria: string (nullable = true)\n",
      " |    |    |-- infoAdicionais: string (nullable = true)\n",
      " |    |    |-- nomeAreaExecutora: string (nullable = true)\n",
      " |    |    |-- origem: string (nullable = true)\n",
      " |    |    |-- paisAreaExecutora: string (nullable = true)\n",
      " |-- idUnico: string (nullable = true)\n",
      " |-- isModeladaPorBim: boolean (nullable = true)\n",
      " |-- metaGlobal: string (nullable = true)\n",
      " |-- natureza: string (nullable = true)\n",
      " |-- naturezaOutras: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- observacoesPertinentes: string (nullable = true)\n",
      " |-- populacaoBeneficiada: string (nullable = true)\n",
      " |-- qdtEmpregosGerados: string (nullable = true)\n",
      " |-- repassadores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- situacao: string (nullable = true)\n",
      " |-- subTipos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- idTipo: long (nullable = true)\n",
      " |-- tipos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- idEixo: long (nullable = true)\n",
      " |-- tomadores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/20 11:25:28 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring, explode, col, to_date, length\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date'\n",
    "process_all = False\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/2024-09-18.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "list_drop = ['tomadores','executores', 'teste']\n",
    "\n",
    "df_eixos = df.drop()\n",
    "df_eixos = df_eixos.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "drop_columns = []\n",
    "\n",
    "for column in df.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "\n",
    "drop_columns\n",
    "\n",
    "#df_eixos = df.drop(*drop_columns)\n",
    "df_eixos.printSchema()\n",
    "#df.schema[5].dataType\n",
    "\n",
    "\n",
    "\n",
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')\n",
    "\n",
    "#print(df.show())\n",
    "#print(df.printSchema())\n",
    "#df.createOrReplaceTempView(\"eixos\")\n",
    "\n",
    "#results = spark.sql(\"select idUnico, eixos.eixos.* from eixos\")\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/20 11:26:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idUnico 11\n",
      "cepAreaExecutora 9\n",
      "datum 11\n",
      "enderecoAreaExecutora 51\n",
      "geometria 42\n",
      "infoAdicionais 92\n",
      "nomeAreaExecutora 60\n",
      "origem 11\n",
      "paisAreaExecutora 6\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Column eixos not found in schema Some(StructType(StructField(idunico,StringType,false),StructField(datacadastro,DateType,false),StructField(nome,StringType,true),StructField(cep,StringType,true),StructField(endereco,StringType,true),StructField(descricao,StringType,true),StructField(funcaosocial,StringType,true),StructField(metaglobal,StringType,true),StructField(datainicialprevista,DateType,true),StructField(datafinalprevista,DateType,true),StructField(datainicialefetiva,DateType,true),StructField(datafinalefetiva,DateType,true),StructField(especie,StringType,true),StructField(natureza,StringType,true),StructField(naturezaoutras,StringType,true),StructField(situacao,StringType,true),StructField(descplanonacionalpoliticavinculado,StringType,true),StructField(uf,StringType,true),StructField(qdtempregosgerados,StringType,true),StructField(descpopulacaobeneficiada,StringType,true),StructField(populacaobeneficiada,StringType,true),StructField(observacoespertinentes,StringType,true),StructField(ismodeladaporbim,BooleanType,true),StructField(datasituacao,DateType,true),StructField(nomearquivo,StringType,true))).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m df_projeto_investimento \u001b[38;5;241m=\u001b[39m df_projeto_investimento\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataCadastro\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataCadastro\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     40\u001b[0m df_projeto_investimento \u001b[38;5;241m=\u001b[39m df_projeto_investimento\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataSituacao\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataSituacao\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 41\u001b[0m \u001b[43mdf_projeto_investimento\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstg_projeto_investimento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m today \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     44\u001b[0m date_before \u001b[38;5;241m=\u001b[39m today \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column eixos not found in schema Some(StructType(StructField(idunico,StringType,false),StructField(datacadastro,DateType,false),StructField(nome,StringType,true),StructField(cep,StringType,true),StructField(endereco,StringType,true),StructField(descricao,StringType,true),StructField(funcaosocial,StringType,true),StructField(metaglobal,StringType,true),StructField(datainicialprevista,DateType,true),StructField(datafinalprevista,DateType,true),StructField(datainicialefetiva,DateType,true),StructField(datafinalefetiva,DateType,true),StructField(especie,StringType,true),StructField(natureza,StringType,true),StructField(naturezaoutras,StringType,true),StructField(situacao,StringType,true),StructField(descplanonacionalpoliticavinculado,StringType,true),StructField(uf,StringType,true),StructField(qdtempregosgerados,StringType,true),StructField(descpopulacaobeneficiada,StringType,true),StructField(populacaobeneficiada,StringType,true),StructField(observacoespertinentes,StringType,true),StructField(ismodeladaporbim,BooleanType,true),StructField(datasituacao,DateType,true),StructField(nomearquivo,StringType,true)))."
     ]
    }
   ],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "mode = 'append'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "\n",
    "df_geometria = df.withColumn(\"geometrias1\", explode(\"geometrias\")).select(\"idUnico\",\"dataCadastro\",\"geometrias1.*\")\n",
    "df_geometria = df_geometria.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_geometria = df_geometria.withColumn(\"dataCriacao\", to_date(col(\"dataCriacao\"), \"yyyy-MM-dd\"))\n",
    "df_geometria = df_geometria.withColumn(\"dataMetadado\", to_date(col(\"dataMetadado\"), \"yyyy-MM-dd\"))\n",
    "df_geometria.write.jdbc(url=url, table='stg_projeto_investimento_geometria', mode=mode, properties=properties)\n",
    "\n",
    "drop_columns = []\n",
    "for column in df_geometria.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "    if type(column.dataType) == StringType:\n",
    "        print(column.name,df_geometria.withColumn(\"len_col\",length(col(column.name))).groupby().max(\"len_col\").head()[0])\n",
    "\n",
    "df_projeto_investimento = df.drop(*drop_columns)\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialPrevista\", to_date(col(\"dataInicialPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalPrevista\", to_date(col(\"dataFinalPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialEfetiva\", to_date(col(\"dataInicialEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalEfetiva\", to_date(col(\"dataFinalEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataSituacao\", to_date(col(\"dataSituacao\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento.write.jdbc(url=url, table='stg_projeto_investimento', mode=mode, properties=properties)\n",
    "\n",
    "today = datetime.now()\n",
    "date_before = today - timedelta(days=30)\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(f'''delete from stg_projeto_investimento_eixos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento               where \"datacadastro\" between '{date_before}' and '{today}';''')\n",
    "\n",
    "#mode = 'append'\n",
    "#properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#df_eixos.write.jdbc(url=url, table='stg_projeto_investimento_fontes_de_recurso', mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-20.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-21.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-22.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-23.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-24.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-25.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-26.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-27.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-28.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-29.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-30.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-01.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-02.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-03.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-04.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-05.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-06.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-07.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-08.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-09.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-10.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-11.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-12.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-13.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-14.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-15.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-16.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-17.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-18.json']\n",
      "delete from stg_projeto_investimento_eixos stg\n",
      "                    using stg_projeto_investimento stg_project\n",
      "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ('RP','SCA');\n",
      "delete from stg_projeto_investimento_tomadores stg\n",
      "                    using stg_projeto_investimento stg_project\n",
      "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ('RP','SCA');\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, explode_outer\n",
    "\n",
    "def generate_dates(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "    return days_date\n",
    "\n",
    "today = datetime.now()\n",
    "dates = generate_dates(today, 30)\n",
    "\n",
    "df = []\n",
    "origins = []\n",
    "\n",
    "for date in dates:\n",
    "    origin_file = origin + f'/{date}.json'\n",
    "    if os.path.isfile(origin_file) == True:\n",
    "        origins.append(origin_file)\n",
    "\n",
    "print(origins)\n",
    "\n",
    "df_novo = spark.read.json(origins)\n",
    "df_novo = df_novo.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "\n",
    "sql_full = ''\n",
    "\n",
    "ufs = ['RP','SCA']\n",
    "\n",
    "tables = ['stg_projeto_investimento_eixos', 'stg_projeto_investimento_tomadores']\n",
    "for table in tables:\n",
    "    sql_full += f'''delete from {table} stg\n",
    "                    using stg_projeto_investimento stg_project\n",
    "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ({\",\".join(\"'\" + uf + \"'\"  for uf in ufs)});\\n'''\n",
    "print(sql_full)\n",
    "\n",
    "\n",
    "def generate_sql_deletion_ufs(tables, ufs):\n",
    "    sql_full = ''\n",
    "    for table in tables:\n",
    "        sql_full += f'''delete from {table} stg\n",
    "                        using stg_projeto_investimento stg_project\n",
    "                        where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.uf in ({\",\".join( \"'\" + uf + \"'\" for uf in ufs)});\\n'''\n",
    "    return sql_full\n",
    "\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(sql_full)\n",
    "curs.execute(f'delete from stg_projeto_investimento where uf in ({\",\".join(\"'\" + uf + \"'\"  for uf in ufs)});')\n",
    "\n",
    "#print( '(' + \",\".join(uf for uf in ufs) + ')')\n",
    "\n",
    "#df += df_novo\n",
    "\n",
    "#df_tomadores = df.withColumn(\"tomadores1\", explode(col(\"tomadores\"))).count()\n",
    "#df_tomadores\n",
    "len(origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "#df.write.jdbc(url=url, table='stg_execucao_financeira', mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Executions: 1\n",
      "Pages: 0\n",
      "N° Registers: 0\n",
      "Errors: 1\n",
      "Errors Consecutives: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 2\n",
      "Pages: 1\n",
      "N° Registers: 99\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 3\n",
      "Pages: 2\n",
      "N° Registers: 198\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 4\n",
      "Pages: 3\n",
      "N° Registers: 298\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 5\n",
      "Pages: 4\n",
      "N° Registers: 398\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 6\n",
      "Pages: 5\n",
      "N° Registers: 497\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 7\n",
      "Pages: 6\n",
      "N° Registers: 597\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 8\n",
      "Pages: 7\n",
      "N° Registers: 696\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 9\n",
      "Pages: 8\n",
      "N° Registers: 796\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 10\n",
      "Pages: 9\n",
      "N° Registers: 896\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 11\n",
      "Pages: 10\n",
      "N° Registers: 994\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 12\n",
      "Pages: 11\n",
      "N° Registers: 1094\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 13\n",
      "Pages: 12\n",
      "N° Registers: 1193\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 14\n",
      "Pages: 13\n",
      "N° Registers: 1293\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 15\n",
      "Pages: 14\n",
      "N° Registers: 1392\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 16\n",
      "Pages: 15\n",
      "N° Registers: 1492\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 17\n",
      "Pages: 16\n",
      "N° Registers: 1590\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 18\n",
      "Pages: 17\n",
      "N° Registers: 1689\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 19\n",
      "Pages: 18\n",
      "N° Registers: 1780\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 20\n",
      "Pages: 19\n",
      "N° Registers: 1880\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Execution Finished with success!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def extract_data_api(url_base, endpoint, uf, dest_path, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    method = \"GET\"\n",
    "    dest_path_file = dest_path + '/' + str(uf) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        url = url_base + endpoint + '?' + 'pagina=' + str(page) + '&' + 'tamanhoDaPagina=' + str(page_size) + '&' + 'uf=' + str(uf)\n",
    "        response = requests.request(method, url)\n",
    "        if response.status_code == 200:\n",
    "            if(len(response.json()[\"content\"]) == 0):\n",
    "                success = True \n",
    "            else:\n",
    "                page += 1\n",
    "                errors_consecutives = 0\n",
    "                executions += 1\n",
    "                content_all += response.json()[\"content\"]\n",
    "                #with open(dest_path_file, 'w') as f:\n",
    "                #    json.dump(response.json()[\"content\"],f)\n",
    "        else:\n",
    "            errors_consecutives += 1\n",
    "            errors += 1\n",
    "            executions += 1\n",
    "            if response.status_code == 429:\n",
    "                time.sleep(1)\n",
    "        print(f'Status Code: {response.status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "page_size = 100\n",
    "url_base = \"https://api.obrasgov.gestao.gov.br\"\n",
    "endpoint = \"/obrasgov/api/projeto-investimento\"\n",
    "current_year = datetime.now().year\n",
    "initial_year = int(current_year)\n",
    "final_year = int(current_year)\n",
    "dest_path = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "errors_limit = - 1\n",
    "errors_consecutives_limit = 5\n",
    "executions_limit = -1\n",
    "\n",
    "extract_data_api(url_base, endpoint, 'PR', dest_path, page_size, errors_limit, errors_consecutives_limit, executions_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-11 10:22:48.977500\n",
      "2024-10-12 10:22:48.977500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "today = datetime.today()\n",
    "days_before = today - timedelta(days=2)\n",
    "\n",
    "\n",
    "date_generated = [days_before + timedelta(days=x) for x in range(0, (today-days_before).days)]\n",
    "\n",
    "for d in date_generated:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(url_base, endpoint, param, method, path_dest_env, fun_api, page_size, errors_limit, errors_consecutives_limit, executions_limit):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    dest_path = os.getenv(path_dest_env)\n",
    "    dest_path_file = dest_path + '/' + str(param) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        success, content,  page, errors_consecutives, errors, executions, status_code =  fun_api(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions)\n",
    "\n",
    "        if content is not None:\n",
    "            content_all.append(content)\n",
    "\n",
    "        print(f'Status Code: {status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "def extract_api_projeto_investimento_uf(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"uf\": str(param)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        if(len(response.json()[\"content\"]) == 0):\n",
    "            success = True\n",
    "            return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "        else:\n",
    "            page += 1\n",
    "            errors_consecutives = 0\n",
    "            executions += 1\n",
    "            content = response.json()[\"content\"]\n",
    "            return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "\n",
    "@task()\n",
    "def extract_data_api_project(url_base, endpoint, date, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    method = \"GET\"\n",
    "    path_dest_env = \"PATH_DEST_PROJETO_INVESTIMENTO\"\n",
    "    extract_api(url_base, endpoint, date, method, path_dest_env, extract_api_projeto_investimento_uf, page_size, errors_limit, errors_consecutives_limit, executions_limit)\n",
    "\n",
    "def extract_api_execucao_financeira_year(url_base, endpoint, year, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"anoFinal\": str(year), \"anoInicial\": str(year)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    success = False\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        page += 1\n",
    "        errors_consecutives = 0\n",
    "        executions += 1\n",
    "        content = response.json()[\"content\"]\n",
    "        return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    elif response.status_code == 404:\n",
    "        success = True\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code \n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13\n",
      "2024-09-14\n",
      "2024-09-15\n",
      "2024-09-16\n",
      "2024-09-17\n",
      "2024-09-18\n",
      "2024-09-19\n",
      "2024-09-20\n",
      "2024-09-21\n",
      "2024-09-22\n",
      "2024-09-23\n",
      "2024-09-24\n",
      "2024-09-25\n",
      "2024-09-26\n",
      "2024-09-27\n",
      "2024-09-28\n",
      "2024-09-29\n",
      "2024-09-30\n",
      "2024-10-01\n",
      "2024-10-02\n",
      "2024-10-03\n",
      "2024-10-04\n",
      "2024-10-05\n",
      "2024-10-06\n",
      "2024-10-07\n",
      "2024-10-08\n",
      "2024-10-09\n",
      "2024-10-10\n",
      "2024-10-11\n",
      "2024-10-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2024-09-13',\n",
       " '2024-09-14',\n",
       " '2024-09-15',\n",
       " '2024-09-16',\n",
       " '2024-09-17',\n",
       " '2024-09-18',\n",
       " '2024-09-19',\n",
       " '2024-09-20',\n",
       " '2024-09-21',\n",
       " '2024-09-22',\n",
       " '2024-09-23',\n",
       " '2024-09-24',\n",
       " '2024-09-25',\n",
       " '2024-09-26',\n",
       " '2024-09-27',\n",
       " '2024-09-28',\n",
       " '2024-09-29',\n",
       " '2024-09-30',\n",
       " '2024-10-01',\n",
       " '2024-10-02',\n",
       " '2024-10-03',\n",
       " '2024-10-04',\n",
       " '2024-10-05',\n",
       " '2024-10-06',\n",
       " '2024-10-07',\n",
       " '2024-10-08',\n",
       " '2024-10-09',\n",
       " '2024-10-10',\n",
       " '2024-10-11',\n",
       " '2024-10-12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dates_reprocessing(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "        print(d.strftime('%Y-%m-%d'))\n",
    "    return days_date\n",
    "\n",
    "generate_dates_reprocessing(today, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
