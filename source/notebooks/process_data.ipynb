{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/16 20:25:44 WARN Utils: Your hostname, Ubuntu-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/10/16 20:25:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/16 20:25:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Extraction_Data\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datetime.now().year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = 2023\n",
    "ano_final = 2024\n",
    "for ano in range(int(ano_inicial), int(ano_final) + 1):\n",
    "    print(ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = current_year\n",
    "ano_final = current_year\n",
    "\n",
    "dif_anos = (ano_final - ano_inicial) + 1\n",
    "for i in range(dif_anos):\n",
    "    print(ano_inicial + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- autorEmenda: string (nullable = true)\n",
      " |-- codigoAmparoLegal: long (nullable = true)\n",
      " |-- descricaoEmpenho: string (nullable = true)\n",
      " |-- fonteRecurso: string (nullable = true)\n",
      " |-- idProjetoInvestimento: string (nullable = true)\n",
      " |-- informacoesComplementares: string (nullable = true)\n",
      " |-- localEntrega: string (nullable = true)\n",
      " |-- naturezaDespesa: string (nullable = true)\n",
      " |-- nomeEsferaOrcamentaria: string (nullable = true)\n",
      " |-- nomeFavorecido: string (nullable = true)\n",
      " |-- nomeTipoEmpenho: string (nullable = true)\n",
      " |-- nrPtres: string (nullable = true)\n",
      " |-- numeroNotaEmpenhoGerada: string (nullable = true)\n",
      " |-- numeroProcesso: string (nullable = true)\n",
      " |-- pagina: long (nullable = true)\n",
      " |-- planoInterno: string (nullable = true)\n",
      " |-- planoOrcamentario: string (nullable = true)\n",
      " |-- resultadoPrimario: string (nullable = true)\n",
      " |-- tamanhoDaPagina: long (nullable = true)\n",
      " |-- tipoCredito: string (nullable = true)\n",
      " |-- ugEmitente: string (nullable = true)\n",
      " |-- ugResponsavel: long (nullable = true)\n",
      " |-- unidadeOrcamentaria: string (nullable = true)\n",
      " |-- valorEmpenho: double (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring\n",
    "\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "process_all = True\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/' + str(ano_inicial)+ '.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "\n",
    "#print(df.show())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cep: string (nullable = true)\n",
      " |-- dataCadastro: string (nullable = true)\n",
      " |-- dataFinalEfetiva: string (nullable = true)\n",
      " |-- dataFinalPrevista: string (nullable = true)\n",
      " |-- dataInicialEfetiva: string (nullable = true)\n",
      " |-- dataInicialPrevista: string (nullable = true)\n",
      " |-- dataSituacao: string (nullable = true)\n",
      " |-- descPlanoNacionalPoliticaVinculado: string (nullable = true)\n",
      " |-- descPopulacaoBeneficiada: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      " |-- endereco: string (nullable = true)\n",
      " |-- especie: string (nullable = true)\n",
      " |-- funcaoSocial: string (nullable = true)\n",
      " |-- idUnico: string (nullable = true)\n",
      " |-- isModeladaPorBim: boolean (nullable = true)\n",
      " |-- metaGlobal: string (nullable = true)\n",
      " |-- natureza: string (nullable = true)\n",
      " |-- naturezaOutras: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- observacoesPertinentes: string (nullable = true)\n",
      " |-- populacaoBeneficiada: string (nullable = true)\n",
      " |-- qdtEmpregosGerados: string (nullable = true)\n",
      " |-- situacao: string (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring, explode, col, to_date, length\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date'\n",
    "process_all = False\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/2024-09-18.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "list_drop = ['tomadores','executores', 'teste']\n",
    "\n",
    "df_eixos = df.drop()\n",
    "df_eixos = df_eixos.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "drop_columns = []\n",
    "\n",
    "for column in df.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "\n",
    "drop_columns\n",
    "\n",
    "df_eixos = df.drop(*drop_columns)\n",
    "df_eixos.printSchema()\n",
    "#df.schema[5].dataType\n",
    "\n",
    "\n",
    "\n",
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')\n",
    "\n",
    "#print(df.show())\n",
    "#print(df.printSchema())\n",
    "#df.createOrReplaceTempView(\"eixos\")\n",
    "\n",
    "#results = spark.sql(\"select idUnico, eixos.eixos.* from eixos\")\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|        NULL|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          53|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|         357|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|        1011|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          70|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          11|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|        1481|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          11|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|         423|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|           7|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|         186|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|         158|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|        NULL|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|           5|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          11|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|           2|\n",
      "+------------+\n",
      "\n",
      "None\n",
      "+------------+\n",
      "|max(len_col)|\n",
      "+------------+\n",
      "|          10|\n",
      "+------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/16 20:36:41 ERROR Executor: Exception in task 0.0 in stage 107.0 (TID 75)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stg_projeto_investimento (\"cep\",\"datacadastro\",\"datafinalefetiva\",\"datafinalprevista\",\"datainicialefetiva\",\"datainicialprevista\",\"datasituacao\",\"descplanonacionalpoliticavinculado\",\"descpopulacaobeneficiada\",\"descricao\",\"endereco\",\"especie\",\"funcaosocial\",\"idunico\",\"ismodeladaporbim\",\"metaglobal\",\"natureza\",\"naturezaoutras\",\"nome\",\"observacoespertinentes\",\"populacaobeneficiada\",\"qdtempregosgerados\",\"situacao\",\"uf\",\"nomearquivo\") VALUES ((NULL),('2024-09-18 -03'::date),(NULL),('2027-12-31 -03'::date),(NULL),('2024-12-31 -03'::date),('2024-09-18 -03'::date),('Politica NAcional deDesenvolvimento Regional - PNDR'),('Familias residentes no perimento urbano, em area de vunerabilidade social, carente que fazem parte do cadastro único em sua maioria familias de baixa renda'),('Execução de obras de Pavimentação no município de Maribondo/Al.'),(NULL),('Construção'),('Será beneficiada uma população aproximadamente de 3000 familias situado no perimtro Urbano do Município de Maribondo/Alagaos. Com a execução de obras de pavimentação no município de Marinbondo, executando serviços de terraplanagem, pavimento, drenagem superficial e sinalização horizontal e vertical além de calcadas promovendo a manutenção e conservação da via, melhorias nas condiçõesde segurança no trânsito e trafegabilidade da via para a circulação de pessoas e veículos e principalmente de condições adequadas de acessibilidade na via para o escoamento de produção da região.. Os resultados esperados são a melhoria da mobilidade, estruturação de atividades produtivas, arranjos produtivos e rotas de integração para promover o desenvolvimento local e territorial, e a melhoria da qualidade de vida da população local, além da geração de emprego e renda.'),('43314.27-68'),('FALSE'::boolean),('Execução de obras de Pavimentação no município de Maribondo/Al.'),('Obra'),(NULL),('PAVIMENTAÇÃO URBANA  EM PARALELEPIPEDO'),(NULL),('3000'),('35'),('Cadastrada'),('AL'),('2024-09-18')) was aborted: ERROR: value too long for type character varying(255)  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(255)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "24/10/16 20:36:41 WARN TaskSetManager: Lost task 0.0 in stage 107.0 (TID 75) (10.0.2.15 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stg_projeto_investimento (\"cep\",\"datacadastro\",\"datafinalefetiva\",\"datafinalprevista\",\"datainicialefetiva\",\"datainicialprevista\",\"datasituacao\",\"descplanonacionalpoliticavinculado\",\"descpopulacaobeneficiada\",\"descricao\",\"endereco\",\"especie\",\"funcaosocial\",\"idunico\",\"ismodeladaporbim\",\"metaglobal\",\"natureza\",\"naturezaoutras\",\"nome\",\"observacoespertinentes\",\"populacaobeneficiada\",\"qdtempregosgerados\",\"situacao\",\"uf\",\"nomearquivo\") VALUES ((NULL),('2024-09-18 -03'::date),(NULL),('2027-12-31 -03'::date),(NULL),('2024-12-31 -03'::date),('2024-09-18 -03'::date),('Politica NAcional deDesenvolvimento Regional - PNDR'),('Familias residentes no perimento urbano, em area de vunerabilidade social, carente que fazem parte do cadastro único em sua maioria familias de baixa renda'),('Execução de obras de Pavimentação no município de Maribondo/Al.'),(NULL),('Construção'),('Será beneficiada uma população aproximadamente de 3000 familias situado no perimtro Urbano do Município de Maribondo/Alagaos. Com a execução de obras de pavimentação no município de Marinbondo, executando serviços de terraplanagem, pavimento, drenagem superficial e sinalização horizontal e vertical além de calcadas promovendo a manutenção e conservação da via, melhorias nas condiçõesde segurança no trânsito e trafegabilidade da via para a circulação de pessoas e veículos e principalmente de condições adequadas de acessibilidade na via para o escoamento de produção da região.. Os resultados esperados são a melhoria da mobilidade, estruturação de atividades produtivas, arranjos produtivos e rotas de integração para promover o desenvolvimento local e territorial, e a melhoria da qualidade de vida da população local, além da geração de emprego e renda.'),('43314.27-68'),('FALSE'::boolean),('Execução de obras de Pavimentação no município de Maribondo/Al.'),('Obra'),(NULL),('PAVIMENTAÇÃO URBANA  EM PARALELEPIPEDO'),(NULL),('3000'),('35'),('Cadastrada'),('AL'),('2024-09-18')) was aborted: ERROR: value too long for type character varying(255)  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(255)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "24/10/16 20:36:41 ERROR TaskSetManager: Task 0 in stage 107.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o574.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 75) (10.0.2.15 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stg_projeto_investimento (\"cep\",\"datacadastro\",\"datafinalefetiva\",\"datafinalprevista\",\"datainicialefetiva\",\"datainicialprevista\",\"datasituacao\",\"descplanonacionalpoliticavinculado\",\"descpopulacaobeneficiada\",\"descricao\",\"endereco\",\"especie\",\"funcaosocial\",\"idunico\",\"ismodeladaporbim\",\"metaglobal\",\"natureza\",\"naturezaoutras\",\"nome\",\"observacoespertinentes\",\"populacaobeneficiada\",\"qdtempregosgerados\",\"situacao\",\"uf\",\"nomearquivo\") VALUES ((NULL),('2024-09-18 -03'::date),(NULL),('2027-12-31 -03'::date),(NULL),('2024-12-31 -03'::date),('2024-09-18 -03'::date),('Politica NAcional deDesenvolvimento Regional - PNDR'),('Familias residentes no perimento urbano, em area de vunerabilidade social, carente que fazem parte do cadastro único em sua maioria familias de baixa renda'),('Execução de obras de Pavimentação no município de Maribondo/Al.'),(NULL),('Construção'),('Será beneficiada uma população aproximadamente de 3000 familias situado no perimtro Urbano do Município de Maribondo/Alagaos. Com a execução de obras de pavimentação no município de Marinbondo, executando serviços de terraplanagem, pavimento, drenagem superficial e sinalização horizontal e vertical além de calcadas promovendo a manutenção e conservação da via, melhorias nas condiçõesde segurança no trânsito e trafegabilidade da via para a circulação de pessoas e veículos e principalmente de condições adequadas de acessibilidade na via para o escoamento de produção da região.. Os resultados esperados são a melhoria da mobilidade, estruturação de atividades produtivas, arranjos produtivos e rotas de integração para promover o desenvolvimento local e territorial, e a melhoria da qualidade de vida da população local, além da geração de emprego e renda.'),('43314.27-68'),('FALSE'::boolean),('Execução de obras de Pavimentação no município de Maribondo/Al.'),('Obra'),(NULL),('PAVIMENTAÇÃO URBANA  EM PARALELEPIPEDO'),(NULL),('3000'),('35'),('Cadastrada'),('AL'),('2024-09-18')) was aborted: ERROR: value too long for type character varying(255)  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(255)\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stg_projeto_investimento (\"cep\",\"datacadastro\",\"datafinalefetiva\",\"datafinalprevista\",\"datainicialefetiva\",\"datainicialprevista\",\"datasituacao\",\"descplanonacionalpoliticavinculado\",\"descpopulacaobeneficiada\",\"descricao\",\"endereco\",\"especie\",\"funcaosocial\",\"idunico\",\"ismodeladaporbim\",\"metaglobal\",\"natureza\",\"naturezaoutras\",\"nome\",\"observacoespertinentes\",\"populacaobeneficiada\",\"qdtempregosgerados\",\"situacao\",\"uf\",\"nomearquivo\") VALUES ((NULL),('2024-09-18 -03'::date),(NULL),('2027-12-31 -03'::date),(NULL),('2024-12-31 -03'::date),('2024-09-18 -03'::date),('Politica NAcional deDesenvolvimento Regional - PNDR'),('Familias residentes no perimento urbano, em area de vunerabilidade social, carente que fazem parte do cadastro único em sua maioria familias de baixa renda'),('Execução de obras de Pavimentação no município de Maribondo/Al.'),(NULL),('Construção'),('Será beneficiada uma população aproximadamente de 3000 familias situado no perimtro Urbano do Município de Maribondo/Alagaos. Com a execução de obras de pavimentação no município de Marinbondo, executando serviços de terraplanagem, pavimento, drenagem superficial e sinalização horizontal e vertical além de calcadas promovendo a manutenção e conservação da via, melhorias nas condiçõesde segurança no trânsito e trafegabilidade da via para a circulação de pessoas e veículos e principalmente de condições adequadas de acessibilidade na via para o escoamento de produção da região.. Os resultados esperados são a melhoria da mobilidade, estruturação de atividades produtivas, arranjos produtivos e rotas de integração para promover o desenvolvimento local e territorial, e a melhoria da qualidade de vida da população local, além da geração de emprego e renda.'),('43314.27-68'),('FALSE'::boolean),('Execução de obras de Pavimentação no município de Maribondo/Al.'),('Obra'),(NULL),('PAVIMENTAÇÃO URBANA  EM PARALELEPIPEDO'),(NULL),('3000'),('35'),('Cadastrada'),('AL'),('2024-09-18')) was aborted: ERROR: value too long for type character varying(255)  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(255)\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m df_projeto_investimento \u001b[38;5;241m=\u001b[39m df_projeto_investimento\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataCadastro\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataCadastro\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     34\u001b[0m df_projeto_investimento \u001b[38;5;241m=\u001b[39m df_projeto_investimento\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataSituacao\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataSituacao\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 35\u001b[0m \u001b[43mdf_projeto_investimento\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstg_projeto_investimento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m today \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     38\u001b[0m date_before \u001b[38;5;241m=\u001b[39m today \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o574.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 75) (10.0.2.15 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stg_projeto_investimento (\"cep\",\"datacadastro\",\"datafinalefetiva\",\"datafinalprevista\",\"datainicialefetiva\",\"datainicialprevista\",\"datasituacao\",\"descplanonacionalpoliticavinculado\",\"descpopulacaobeneficiada\",\"descricao\",\"endereco\",\"especie\",\"funcaosocial\",\"idunico\",\"ismodeladaporbim\",\"metaglobal\",\"natureza\",\"naturezaoutras\",\"nome\",\"observacoespertinentes\",\"populacaobeneficiada\",\"qdtempregosgerados\",\"situacao\",\"uf\",\"nomearquivo\") VALUES ((NULL),('2024-09-18 -03'::date),(NULL),('2027-12-31 -03'::date),(NULL),('2024-12-31 -03'::date),('2024-09-18 -03'::date),('Politica NAcional deDesenvolvimento Regional - PNDR'),('Familias residentes no perimento urbano, em area de vunerabilidade social, carente que fazem parte do cadastro único em sua maioria familias de baixa renda'),('Execução de obras de Pavimentação no município de Maribondo/Al.'),(NULL),('Construção'),('Será beneficiada uma população aproximadamente de 3000 familias situado no perimtro Urbano do Município de Maribondo/Alagaos. Com a execução de obras de pavimentação no município de Marinbondo, executando serviços de terraplanagem, pavimento, drenagem superficial e sinalização horizontal e vertical além de calcadas promovendo a manutenção e conservação da via, melhorias nas condiçõesde segurança no trânsito e trafegabilidade da via para a circulação de pessoas e veículos e principalmente de condições adequadas de acessibilidade na via para o escoamento de produção da região.. Os resultados esperados são a melhoria da mobilidade, estruturação de atividades produtivas, arranjos produtivos e rotas de integração para promover o desenvolvimento local e territorial, e a melhoria da qualidade de vida da população local, além da geração de emprego e renda.'),('43314.27-68'),('FALSE'::boolean),('Execução de obras de Pavimentação no município de Maribondo/Al.'),('Obra'),(NULL),('PAVIMENTAÇÃO URBANA  EM PARALELEPIPEDO'),(NULL),('3000'),('35'),('Cadastrada'),('AL'),('2024-09-18')) was aborted: ERROR: value too long for type character varying(255)  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(255)\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stg_projeto_investimento (\"cep\",\"datacadastro\",\"datafinalefetiva\",\"datafinalprevista\",\"datainicialefetiva\",\"datainicialprevista\",\"datasituacao\",\"descplanonacionalpoliticavinculado\",\"descpopulacaobeneficiada\",\"descricao\",\"endereco\",\"especie\",\"funcaosocial\",\"idunico\",\"ismodeladaporbim\",\"metaglobal\",\"natureza\",\"naturezaoutras\",\"nome\",\"observacoespertinentes\",\"populacaobeneficiada\",\"qdtempregosgerados\",\"situacao\",\"uf\",\"nomearquivo\") VALUES ((NULL),('2024-09-18 -03'::date),(NULL),('2027-12-31 -03'::date),(NULL),('2024-12-31 -03'::date),('2024-09-18 -03'::date),('Politica NAcional deDesenvolvimento Regional - PNDR'),('Familias residentes no perimento urbano, em area de vunerabilidade social, carente que fazem parte do cadastro único em sua maioria familias de baixa renda'),('Execução de obras de Pavimentação no município de Maribondo/Al.'),(NULL),('Construção'),('Será beneficiada uma população aproximadamente de 3000 familias situado no perimtro Urbano do Município de Maribondo/Alagaos. Com a execução de obras de pavimentação no município de Marinbondo, executando serviços de terraplanagem, pavimento, drenagem superficial e sinalização horizontal e vertical além de calcadas promovendo a manutenção e conservação da via, melhorias nas condiçõesde segurança no trânsito e trafegabilidade da via para a circulação de pessoas e veículos e principalmente de condições adequadas de acessibilidade na via para o escoamento de produção da região.. Os resultados esperados são a melhoria da mobilidade, estruturação de atividades produtivas, arranjos produtivos e rotas de integração para promover o desenvolvimento local e territorial, e a melhoria da qualidade de vida da população local, além da geração de emprego e renda.'),('43314.27-68'),('FALSE'::boolean),('Execução de obras de Pavimentação no município de Maribondo/Al.'),('Obra'),(NULL),('PAVIMENTAÇÃO URBANA  EM PARALELEPIPEDO'),(NULL),('3000'),('35'),('Cadastrada'),('AL'),('2024-09-18')) was aborted: ERROR: value too long for type character varying(255)  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(255)\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "mode = 'append'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "\n",
    "drop_columns = []\n",
    "for column in df.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "    if type(column.dataType) == StringType:\n",
    "        print(df.withColumn(\"len_col\",length(col(column.name))).groupby().max(\"len_col\").show())\n",
    "\n",
    "df_projeto_investimento = df.drop(*drop_columns)\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialPrevista\", to_date(col(\"dataInicialPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalPrevista\", to_date(col(\"dataFinalPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialEfetiva\", to_date(col(\"dataInicialEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalEfetiva\", to_date(col(\"dataFinalEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataSituacao\", to_date(col(\"dataSituacao\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento.write.jdbc(url=url, table='stg_projeto_investimento', mode=mode, properties=properties)\n",
    "\n",
    "today = datetime.now()\n",
    "date_before = today - timedelta(days=30)\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(f'''delete from stg_projeto_investimento_eixos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento               where \"datacadastro\" between '{date_before}' and '{today}';''')\n",
    "\n",
    "#mode = 'append'\n",
    "#properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#df_eixos.write.jdbc(url=url, table='stg_projeto_investimento_fontes_de_recurso', mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-09-13T13:31:02.426132'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_before.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "#df.write.jdbc(url=url, table='stg_execucao_financeira', mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Executions: 1\n",
      "Pages: 0\n",
      "N° Registers: 0\n",
      "Errors: 1\n",
      "Errors Consecutives: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 2\n",
      "Pages: 1\n",
      "N° Registers: 99\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 3\n",
      "Pages: 2\n",
      "N° Registers: 198\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 4\n",
      "Pages: 3\n",
      "N° Registers: 298\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 5\n",
      "Pages: 4\n",
      "N° Registers: 398\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 6\n",
      "Pages: 5\n",
      "N° Registers: 497\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 7\n",
      "Pages: 6\n",
      "N° Registers: 597\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 8\n",
      "Pages: 7\n",
      "N° Registers: 696\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 9\n",
      "Pages: 8\n",
      "N° Registers: 796\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 10\n",
      "Pages: 9\n",
      "N° Registers: 896\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 11\n",
      "Pages: 10\n",
      "N° Registers: 994\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 12\n",
      "Pages: 11\n",
      "N° Registers: 1094\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 13\n",
      "Pages: 12\n",
      "N° Registers: 1193\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 14\n",
      "Pages: 13\n",
      "N° Registers: 1293\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 15\n",
      "Pages: 14\n",
      "N° Registers: 1392\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 16\n",
      "Pages: 15\n",
      "N° Registers: 1492\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 17\n",
      "Pages: 16\n",
      "N° Registers: 1590\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 18\n",
      "Pages: 17\n",
      "N° Registers: 1689\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 19\n",
      "Pages: 18\n",
      "N° Registers: 1780\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 20\n",
      "Pages: 19\n",
      "N° Registers: 1880\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Execution Finished with success!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def extract_data_api(url_base, endpoint, uf, dest_path, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    method = \"GET\"\n",
    "    dest_path_file = dest_path + '/' + str(uf) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        url = url_base + endpoint + '?' + 'pagina=' + str(page) + '&' + 'tamanhoDaPagina=' + str(page_size) + '&' + 'uf=' + str(uf)\n",
    "        response = requests.request(method, url)\n",
    "        if response.status_code == 200:\n",
    "            if(len(response.json()[\"content\"]) == 0):\n",
    "                success = True \n",
    "            else:\n",
    "                page += 1\n",
    "                errors_consecutives = 0\n",
    "                executions += 1\n",
    "                content_all += response.json()[\"content\"]\n",
    "                #with open(dest_path_file, 'w') as f:\n",
    "                #    json.dump(response.json()[\"content\"],f)\n",
    "        else:\n",
    "            errors_consecutives += 1\n",
    "            errors += 1\n",
    "            executions += 1\n",
    "            if response.status_code == 429:\n",
    "                time.sleep(1)\n",
    "        print(f'Status Code: {response.status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "page_size = 100\n",
    "url_base = \"https://api.obrasgov.gestao.gov.br\"\n",
    "endpoint = \"/obrasgov/api/projeto-investimento\"\n",
    "current_year = datetime.now().year\n",
    "initial_year = int(current_year)\n",
    "final_year = int(current_year)\n",
    "dest_path = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "errors_limit = - 1\n",
    "errors_consecutives_limit = 5\n",
    "executions_limit = -1\n",
    "\n",
    "extract_data_api(url_base, endpoint, 'PR', dest_path, page_size, errors_limit, errors_consecutives_limit, executions_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-11 10:22:48.977500\n",
      "2024-10-12 10:22:48.977500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "today = datetime.today()\n",
    "days_before = today - timedelta(days=2)\n",
    "\n",
    "\n",
    "date_generated = [days_before + timedelta(days=x) for x in range(0, (today-days_before).days)]\n",
    "\n",
    "for d in date_generated:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(url_base, endpoint, param, method, path_dest_env, fun_api, page_size, errors_limit, errors_consecutives_limit, executions_limit):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    dest_path = os.getenv(path_dest_env)\n",
    "    dest_path_file = dest_path + '/' + str(param) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        success, content,  page, errors_consecutives, errors, executions, status_code =  fun_api(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions)\n",
    "\n",
    "        if content is not None:\n",
    "            content_all.append(content)\n",
    "\n",
    "        print(f'Status Code: {status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "def extract_api_projeto_investimento_uf(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"uf\": str(param)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        if(len(response.json()[\"content\"]) == 0):\n",
    "            success = True\n",
    "            return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "        else:\n",
    "            page += 1\n",
    "            errors_consecutives = 0\n",
    "            executions += 1\n",
    "            content = response.json()[\"content\"]\n",
    "            return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "\n",
    "@task()\n",
    "def extract_data_api_project(url_base, endpoint, date, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    method = \"GET\"\n",
    "    path_dest_env = \"PATH_DEST_PROJETO_INVESTIMENTO\"\n",
    "    extract_api(url_base, endpoint, date, method, path_dest_env, extract_api_projeto_investimento_uf, page_size, errors_limit, errors_consecutives_limit, executions_limit)\n",
    "\n",
    "def extract_api_execucao_financeira_year(url_base, endpoint, year, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"anoFinal\": str(year), \"anoInicial\": str(year)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    success = False\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        page += 1\n",
    "        errors_consecutives = 0\n",
    "        executions += 1\n",
    "        content = response.json()[\"content\"]\n",
    "        return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    elif response.status_code == 404:\n",
    "        success = True\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code \n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13\n",
      "2024-09-14\n",
      "2024-09-15\n",
      "2024-09-16\n",
      "2024-09-17\n",
      "2024-09-18\n",
      "2024-09-19\n",
      "2024-09-20\n",
      "2024-09-21\n",
      "2024-09-22\n",
      "2024-09-23\n",
      "2024-09-24\n",
      "2024-09-25\n",
      "2024-09-26\n",
      "2024-09-27\n",
      "2024-09-28\n",
      "2024-09-29\n",
      "2024-09-30\n",
      "2024-10-01\n",
      "2024-10-02\n",
      "2024-10-03\n",
      "2024-10-04\n",
      "2024-10-05\n",
      "2024-10-06\n",
      "2024-10-07\n",
      "2024-10-08\n",
      "2024-10-09\n",
      "2024-10-10\n",
      "2024-10-11\n",
      "2024-10-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2024-09-13',\n",
       " '2024-09-14',\n",
       " '2024-09-15',\n",
       " '2024-09-16',\n",
       " '2024-09-17',\n",
       " '2024-09-18',\n",
       " '2024-09-19',\n",
       " '2024-09-20',\n",
       " '2024-09-21',\n",
       " '2024-09-22',\n",
       " '2024-09-23',\n",
       " '2024-09-24',\n",
       " '2024-09-25',\n",
       " '2024-09-26',\n",
       " '2024-09-27',\n",
       " '2024-09-28',\n",
       " '2024-09-29',\n",
       " '2024-09-30',\n",
       " '2024-10-01',\n",
       " '2024-10-02',\n",
       " '2024-10-03',\n",
       " '2024-10-04',\n",
       " '2024-10-05',\n",
       " '2024-10-06',\n",
       " '2024-10-07',\n",
       " '2024-10-08',\n",
       " '2024-10-09',\n",
       " '2024-10-10',\n",
       " '2024-10-11',\n",
       " '2024-10-12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dates_reprocessing(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "        print(d.strftime('%Y-%m-%d'))\n",
    "    return days_date\n",
    "\n",
    "generate_dates_reprocessing(today, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
