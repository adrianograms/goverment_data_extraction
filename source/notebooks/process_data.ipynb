{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExtraction_Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.extraClassPath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Extraction_Data\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datetime.now().year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = 2023\n",
    "ano_final = 2024\n",
    "for ano in range(int(ano_inicial), int(ano_final) + 1):\n",
    "    print(ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = current_year\n",
    "ano_final = current_year\n",
    "\n",
    "dif_anos = (ano_final - ano_inicial) + 1\n",
    "for i in range(dif_anos):\n",
    "    print(ano_inicial + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- autorEmenda: string (nullable = true)\n",
      " |-- codigoAmparoLegal: long (nullable = true)\n",
      " |-- descricaoEmpenho: string (nullable = true)\n",
      " |-- fonteRecurso: string (nullable = true)\n",
      " |-- idProjetoInvestimento: string (nullable = true)\n",
      " |-- informacoesComplementares: string (nullable = true)\n",
      " |-- localEntrega: string (nullable = true)\n",
      " |-- naturezaDespesa: string (nullable = true)\n",
      " |-- nomeEsferaOrcamentaria: string (nullable = true)\n",
      " |-- nomeFavorecido: string (nullable = true)\n",
      " |-- nomeTipoEmpenho: string (nullable = true)\n",
      " |-- nrPtres: string (nullable = true)\n",
      " |-- numeroNotaEmpenhoGerada: string (nullable = true)\n",
      " |-- numeroProcesso: string (nullable = true)\n",
      " |-- pagina: long (nullable = true)\n",
      " |-- planoInterno: string (nullable = true)\n",
      " |-- planoOrcamentario: string (nullable = true)\n",
      " |-- resultadoPrimario: string (nullable = true)\n",
      " |-- tamanhoDaPagina: long (nullable = true)\n",
      " |-- tipoCredito: string (nullable = true)\n",
      " |-- ugEmitente: string (nullable = true)\n",
      " |-- ugResponsavel: long (nullable = true)\n",
      " |-- unidadeOrcamentaria: string (nullable = true)\n",
      " |-- valorEmpenho: double (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring\n",
    "\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "process_all = True\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/' + str(ano_inicial)+ '.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "\n",
    "#print(df.show())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cep: string (nullable = true)\n",
      " |-- dataCadastro: date (nullable = true)\n",
      " |-- dataFinalEfetiva: string (nullable = true)\n",
      " |-- dataFinalPrevista: string (nullable = true)\n",
      " |-- dataInicialEfetiva: string (nullable = true)\n",
      " |-- dataInicialPrevista: string (nullable = true)\n",
      " |-- dataSituacao: string (nullable = true)\n",
      " |-- descPlanoNacionalPoliticaVinculado: string (nullable = true)\n",
      " |-- descPopulacaoBeneficiada: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      " |-- eixos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |-- endereco: string (nullable = true)\n",
      " |-- especie: string (nullable = true)\n",
      " |-- executores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- fontesDeRecurso: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- origem: string (nullable = true)\n",
      " |    |    |-- valorInvestimentoPrevisto: double (nullable = true)\n",
      " |-- funcaoSocial: string (nullable = true)\n",
      " |-- geometrias: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- cepAreaExecutora: string (nullable = true)\n",
      " |    |    |-- dataCriacao: string (nullable = true)\n",
      " |    |    |-- dataMetadado: string (nullable = true)\n",
      " |    |    |-- datum: string (nullable = true)\n",
      " |    |    |-- enderecoAreaExecutora: string (nullable = true)\n",
      " |    |    |-- geometria: string (nullable = true)\n",
      " |    |    |-- infoAdicionais: string (nullable = true)\n",
      " |    |    |-- nomeAreaExecutora: string (nullable = true)\n",
      " |    |    |-- origem: string (nullable = true)\n",
      " |    |    |-- paisAreaExecutora: string (nullable = true)\n",
      " |-- idUnico: string (nullable = true)\n",
      " |-- isModeladaPorBim: boolean (nullable = true)\n",
      " |-- metaGlobal: string (nullable = true)\n",
      " |-- natureza: string (nullable = true)\n",
      " |-- naturezaOutras: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- observacoesPertinentes: string (nullable = true)\n",
      " |-- populacaoBeneficiada: string (nullable = true)\n",
      " |-- qdtEmpregosGerados: string (nullable = true)\n",
      " |-- repassadores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- situacao: string (nullable = true)\n",
      " |-- subTipos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- idTipo: long (nullable = true)\n",
      " |-- tipos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- idEixo: long (nullable = true)\n",
      " |-- tomadores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring, explode, col, to_date, length\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date'\n",
    "process_all = False\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/2024-09-18.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "list_drop = ['tomadores','executores', 'teste']\n",
    "\n",
    "df_eixos = df.drop()\n",
    "df_eixos = df_eixos.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "drop_columns = []\n",
    "\n",
    "for column in df.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "\n",
    "drop_columns\n",
    "\n",
    "#df_eixos = df.drop(*drop_columns)\n",
    "df_eixos.printSchema()\n",
    "#df.schema[5].dataType\n",
    "\n",
    "\n",
    "\n",
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')\n",
    "\n",
    "#print(df.show())\n",
    "#print(df.printSchema())\n",
    "#df.createOrReplaceTempView(\"eixos\")\n",
    "\n",
    "#results = spark.sql(\"select idUnico, eixos.eixos.* from eixos\")\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 10:21:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/10/27 10:21:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idUnico 11\n",
      "cepAreaExecutora 9\n",
      "datum 11\n",
      "enderecoAreaExecutora 51\n",
      "geometria 42\n",
      "infoAdicionais 92\n",
      "nomeAreaExecutora 60\n",
      "origem 11\n",
      "paisAreaExecutora 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf_projeto_investimento = df.drop(*drop_columns)\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialPrevista\", to_date(col(\"dataInicialPrevista\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalPrevista\", to_date(col(\"dataFinalPrevista\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialEfetiva\", to_date(col(\"dataInicialEfetiva\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalEfetiva\", to_date(col(\"dataFinalEfetiva\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataSituacao\", to_date(col(\"dataSituacao\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento.write.jdbc(url=url, table=\\'stg_projeto_investimento\\', mode=mode, properties=properties)\\n\\ntoday = datetime.now()\\ndate_before = today - timedelta(days=30)\\n\\nmode = \\'append\\'\\nurl = f\\'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}\\'\\nproperties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\\nconn = jaydebeapi.connect(driver, url, [user_dw, password_dw], \\'/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\\')\\ncurs = conn.cursor()\\ncurs.execute(f\\'\\'\\'delete from stg_projeto_investimento_eixos         where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento               where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\'\\'\\')\\n\\n#mode = \\'append\\'\\n#properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\\n#df_eixos.write.jdbc(url=url, table=\\'stg_projeto_investimento_fontes_de_recurso\\', mode=mode, properties=properties)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "mode = 'append'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "\n",
    "df_geometria = df.withColumn(\"geometrias1\", explode(\"geometrias\")).select(\"idUnico\",\"dataCadastro\",\"geometrias1.*\")\n",
    "df_geometria = df_geometria.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_geometria = df_geometria.withColumn(\"dataCriacao\", to_date(col(\"dataCriacao\"), \"yyyy-MM-dd\"))\n",
    "df_geometria = df_geometria.withColumn(\"dataMetadado\", to_date(col(\"dataMetadado\"), \"yyyy-MM-dd\"))\n",
    "df_geometria.write.jdbc(url=url, table='stg_projeto_investimento_geometria', mode=mode, properties=properties)\n",
    "\n",
    "drop_columns = []\n",
    "for column in df_geometria.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "    if type(column.dataType) == StringType:\n",
    "        print(column.name,df_geometria.withColumn(\"len_col\",length(col(column.name))).groupby().max(\"len_col\").head()[0])\n",
    "\n",
    "\"\"\"\n",
    "df_projeto_investimento = df.drop(*drop_columns)\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialPrevista\", to_date(col(\"dataInicialPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalPrevista\", to_date(col(\"dataFinalPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialEfetiva\", to_date(col(\"dataInicialEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalEfetiva\", to_date(col(\"dataFinalEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataSituacao\", to_date(col(\"dataSituacao\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento.write.jdbc(url=url, table='stg_projeto_investimento', mode=mode, properties=properties)\n",
    "\n",
    "today = datetime.now()\n",
    "date_before = today - timedelta(days=30)\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(f'''delete from stg_projeto_investimento_eixos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento               where \"datacadastro\" between '{date_before}' and '{today}';''')\n",
    "\n",
    "#mode = 'append'\n",
    "#properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#df_eixos.write.jdbc(url=url, table='stg_projeto_investimento_fontes_de_recurso', mode=mode, properties=properties)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nk_eixo: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cast\n",
    "\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "\n",
    "df_eixos = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select distinct cast(id as varchar) as nk_eixo, descricao from stg_projeto_investimento_eixos\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load()\n",
    "\n",
    "df_eixos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2', 'Militar'),\n",
       " ('1', 'Administrativo'),\n",
       " ('3', 'Econômico'),\n",
       " ('4', 'Social')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df_eixos.rdd\n",
    "data = rdd.map(tuple)\n",
    "\n",
    "data = data.collect()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|nk_eixo|        descricao|\n",
      "+-------+-----------------+\n",
      "|      1|Administrativoabc|\n",
      "|      3|     Econômicoabc|\n",
      "|      4|           Social|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Militar\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "from pyspark.sql.functions import udf \n",
    "\n",
    "@udf(StringType())\n",
    "def concat_names(column):\n",
    "    print(column)\n",
    "    if column not in ('Militar', 'Social'):\n",
    "        abc = column + 'abc'\n",
    "        return abc\n",
    "    return column\n",
    "\n",
    "df_eixos_2 = df_eixos\n",
    "df_eixos_2 = df_eixos_2.withColumn('descricao', concat_names('descricao'))\n",
    "df_eixos_2 = df_eixos_2.filter(\"descricao != 'Militar'\")\n",
    "df_eixos_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Militar\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|nk_eixo|        descricao|\n",
      "+-------+-----------------+\n",
      "|      1|Administrativoabc|\n",
      "|      3|     Econômicoabc|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "from functools import reduce\n",
    "\n",
    "def values_to_update(df_old, df_new, key_columns, compare_columns):\n",
    "    \"\"\"\n",
    "    Compared two dataframes and find the rows to update.\n",
    "\n",
    "    :param df_old: Old DataFrame\n",
    "    :param df_new: New DataFrame\n",
    "    :param key_columns: Join columns\n",
    "    :param compare_columns: Comparisson coluns\n",
    "    :return: Update DataFrame\n",
    "    \"\"\"\n",
    "    # Criar a condição de join\n",
    "    join_condition = [col(f\"new.{key}\") == col(f\"old.{key}\") for key in key_columns]\n",
    "    \n",
    "    # Realizar o join\n",
    "    joined_df = df_new.alias(\"new\").join(df_old.alias(\"old\"), on=join_condition, how=\"inner\")\n",
    "\n",
    "    # Filtrar as diferenças\n",
    "    # filter_condition = [\n",
    "    #    (col(f\"new.{column}\") != col(f\"old.{column}\")) for column in compare_columns\n",
    "    #]\n",
    "\n",
    "    filter_condition = [\n",
    "        f'not new.{column} <=> old.{column}' for column in compare_columns\n",
    "    ]\n",
    "\n",
    "    # Aplicar o filtro\n",
    "    #differences = joined_df.filter(reduce(lambda a, b: a | b, filter_condition))\n",
    "    differences = joined_df.filter(' or '.join(filter_condition))\n",
    "    \n",
    "    return differences.select(\"new.*\")\n",
    "\n",
    "def values_to_insert_delete(df1, df2, key_columns):\n",
    "    \"\"\"\n",
    "    Compared two dataframes and find the rows to update.\n",
    "\n",
    "    :param df1: Secondary DataFrame\n",
    "    :param df2: Main DataFrame\n",
    "    :param key_columns: Join columns\n",
    "    :return: Insert Or Delete DataFrame\n",
    "    \"\"\"\n",
    "    # Criar a condição de join\n",
    "    join_condition = [col(f\"new.{key}\") == col(f\"old.{key}\") for key in key_columns]\n",
    "    \n",
    "    # Realizar o join\n",
    "    joined_df = df2.alias(\"new\").join(df1.alias(\"old\"), on=join_condition, how=\"leftanti\")\n",
    "\n",
    "    return joined_df.select(\"new.*\")\n",
    "\n",
    "def values_to_delete(df1, df2, key_columns):\n",
    "    \"\"\"\n",
    "    Compara dois DataFrames do Spark com base em colunas chave e colunas a serem comparadas.\n",
    "\n",
    "    :param df1: Primeiro DataFrame\n",
    "    :param df2: Segundo DataFrame\n",
    "    :param key_columns: Lista de colunas chave para o join\n",
    "    :param compare_columns: Lista de colunas a serem comparadas\n",
    "    :return: DataFrame contendo as diferenças\n",
    "    \"\"\"\n",
    "    # Criar a condição de join\n",
    "    join_condition = [col(f\"new.{key}\") == col(f\"old.{key}\") for key in key_columns]\n",
    "    \n",
    "    # Realizar o join\n",
    "    joined_df = df1.alias(\"new\").join(df2.alias(\"old\"), on=join_condition, how=\"leftanti\")\n",
    "\n",
    "    return joined_df.select(\"new.*\")\n",
    "\n",
    "teste = values_to_update(df_eixos, df_eixos_2, ['nk_eixo'], ['descricao'])\n",
    "\n",
    "#teste = values_to_insert(df_eixos, df_eixos_2, ['nk_eixo'])\n",
    "\n",
    "#teste = values_to_delete(df_eixos, df_eixos_2, ['nk_eixo'])\n",
    " \n",
    "teste.show()\n",
    "\n",
    "#conditions_ = len([when(df_eixos[c]!=df_eixos_2[c], lit(c)).otherwise(\"\") for c in df_eixos.columns if c != 'nk_eixo']) == 0\n",
    "\n",
    "#df_eixos.join(df_eixos_2, ['nk_eixo'], how='outer').select(df_eixos.descricao, df_eixos_2.descricao).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|nk_eixo|     descricao|\n",
      "+-------+--------------+\n",
      "|      2|       Militar|\n",
      "|      1|Administrativo|\n",
      "|      3|     Econômico|\n",
      "+-------+--------------+\n",
      "\n",
      "None\n",
      "+-------+---------+\n",
      "|nk_eixo|descricao|\n",
      "+-------+---------+\n",
      "|      4|   Social|\n",
      "+-------+---------+\n",
      "\n",
      "None\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nk_eixo', 'descricao']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "bulk_size = 3\n",
    "split_w = ceil(df_eixos.count() / bulk_size)\n",
    "df_result = df_eixos\n",
    "for i in range(split_w):\n",
    "    df_eixos_limited = df_result.limit(bulk_size)\n",
    "    df_result = df_result.subtract(df_eixos_limited)\n",
    "    print(df_eixos_limited.show())\n",
    "\n",
    "print(split_w)\n",
    "\n",
    "df_eixos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  psycopg2 \n",
    "from math import ceil\n",
    "\n",
    "f\"dbname='{database_dw}' user='{properties['user']}' password='{properties['password']}' host='{host_dw}' port='{port_dw}'\"\n",
    "\n",
    "def create_sql(table_name, columns, key_columns, type):\n",
    "    sql = ''\n",
    "    if type in ('DELETE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        sql = f'DELETE FROM {table_name} WHERE {columns_comparisson}'\n",
    "    elif type in ('INSERT'):\n",
    "        columns_str = list(map(lambda x: '\"' + x + '\"',columns + key_columns))\n",
    "        columns_str = ', '.join(columns_str)\n",
    "        placeholders = ', '.join(['%s'] * len(columns + key_columns))\n",
    "        sql = f'INSERT INTO {table_name}({columns_str}) VALUES({placeholders})'\n",
    "    elif type in ('UPDATE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        columns_set = [f'\"{column}\" = %s' for column in columns]\n",
    "        columns_set = ', '.join(columns_set)\n",
    "        sql = f'UPDATE {table_name} SET {columns_set} WHERE {columns_comparisson}'\n",
    "\n",
    "    return sql\n",
    "\n",
    "def create_connection(connection_properties): \n",
    "    #Connect to the Postgresql database using the psycopg2 adapter. \n",
    "    #Pass your database name , username , password , hostname and port number \n",
    "    conn = psycopg2.connect(f\"dbname='{connection_properties['db_name']}' user='{connection_properties['user']}' password='{connection_properties['password']}'\\\n",
    "                            host='{connection_properties['host']}' port='{connection_properties['port']}'\") \n",
    "    #Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn,curr \n",
    "\n",
    "def bulk_values(table_name, df, bulk_size, connection_properties, type, columns, key_columns=[]):\n",
    "\n",
    "    if df.count() == 0:\n",
    "        return\n",
    "\n",
    "    splits = ceil(df.count() / bulk_size)\n",
    "    conn, curr = create_connection(connection_properties)\n",
    "    rdn_splits = [float(bulk_size)] * splits\n",
    "    splits_dfs = df.randomSplit(rdn_splits)\n",
    "\n",
    "\n",
    "    for split_index, split_df in enumerate(splits_dfs):\n",
    "\n",
    "        if split_df.count() == 0:\n",
    "            continue\n",
    "\n",
    "        query = create_sql(table_name, columns, key_columns, type)\n",
    "\n",
    "        if type in ('UPDATE', 'INSERT'):\n",
    "            split_df = split_df.select(columns + key_columns)\n",
    "        elif type in ('DELETE'):\n",
    "            split_df = split_df.select(key_columns)\n",
    "\n",
    "\n",
    "        rdd = split_df.rdd\n",
    "        data = rdd.map(tuple)\n",
    "        data = data.collect()\n",
    "\n",
    "\n",
    "        curr.executemany(query, data) \n",
    "        conn.commit() \n",
    "\n",
    "        print(f'Split: {split_index + 1}\\\\{splits}\\\n",
    "                Bulk Size: {split_df.count()}')\n",
    "        split_df.unpersist()\n",
    "\n",
    "    conn.close() \n",
    "\n",
    "df_execucao_fin = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from public.stg_projeto_investimento\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load()\n",
    "\n",
    "table_name = 'stg_projeto_investimento_teste'\n",
    "connection_properties = {'db_name':database_dw, 'user':user_dw, 'password':password_dw, 'host':host_dw, 'port':port_dw}\n",
    "\n",
    "#bulk_values(table_name, df_execucao_fin, 500, connection_properties, 'UPDATE' ,df_execucao_fin.columns, ['idunico'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o26.read",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_project_inv \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from public.stg_projeto_investimento\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m      9\u001b[0m df_project_inv_teste \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)\\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from public.stg_projeto_investimento_teste\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrud_database\u001b[39m(df_old, df_new, table_name, key_columns, connection_properties, bulk_size, insert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, delete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/session.py:1706\u001b[0m, in \u001b[0;36mSparkSession.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameReader:\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;124;03m    +---+------------+\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/readwriter.py:70\u001b[0m, in \u001b[0;36mDataFrameReader.__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m spark\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o26.read"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "df_project_inv = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from public.stg_projeto_investimento\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load().cache()\n",
    "\n",
    "df_project_inv_teste = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from public.stg_projeto_investimento_teste\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load().cache()\n",
    "\n",
    "def crud_database(df_old, df_new, table_name, key_columns, connection_properties, bulk_size, insert = True, update = True, delete = False):\n",
    "    columns_comparisson = df_new.columns\n",
    "    columns_comparisson = [column for column in columns_comparisson if not column in key_columns] \n",
    "\n",
    "    if delete == True:\n",
    "        df_delete = values_to_insert_delete(df_new, df_old, key_columns)\n",
    "        print('Starting delete...')\n",
    "        bulk_values(table_name, df_delete, bulk_size, connection_properties, 'DELETE' ,columns_comparisson, key_columns)\n",
    "        print(f'Deleted {df_delete.count()} rows')\n",
    "        df_delete.unpersist()\n",
    "\n",
    "    if update == True:\n",
    "        df_update = values_to_update(df_old, df_new, key_columns, columns_comparisson)\n",
    "        print('Starting update...')\n",
    "        bulk_values(table_name, df_update, bulk_size, connection_properties, 'UPDATE' ,columns_comparisson, key_columns)\n",
    "        print(f'Updated {df_update.count()} rows')\n",
    "        df_update.unpersist()\n",
    "\n",
    "    if insert == True:\n",
    "        df_insert = values_to_insert_delete(df_old, df_new, key_columns)\n",
    "        print('Starting insert...')\n",
    "        bulk_values(table_name, df_insert, bulk_size, connection_properties, 'INSERT' ,columns_comparisson, key_columns)\n",
    "        print(f'Inserted {df_insert.count()} rows')\n",
    "        df_insert.unpersist()\n",
    "\n",
    "table_name = 'public.stg_projeto_investimento_teste'\n",
    "key_columns = ['idunico']\n",
    "connection_properties = {'db_name':database_dw, 'user':user_dw, 'password':password_dw, 'host':host_dw, 'port':port_dw}\n",
    "\n",
    "columns_comparisson = df_project_inv_teste.columns\n",
    "columns_comparisson = [column for column in columns_comparisson if not column in key_columns] \n",
    "\n",
    "crud_database(df_project_inv_teste, df_project_inv , table_name, key_columns, connection_properties, 500, delete=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#key_columns = ['idunico']\n",
    "#columns_comparisson = df_project_inv.columns\n",
    "\n",
    "#values_to_update(df_project_inv,df_project_inv_teste,key_columns,columns_comparisson).show()\n",
    "\n",
    "#df_project_inv_teste.filter('idunico <=> \"13414.41-30\"').select('qdtempregosgerados').show()\n",
    "\n",
    "#[column for column in columns_comparisson if not column in key_columns or key_columns.remove(column)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+\n",
      "|     descricao|nk_eixo|nk_eixo|\n",
      "+--------------+-------+-------+\n",
      "|       Militar|      2|      2|\n",
      "|Administrativo|      1|      1|\n",
      "|     Econômico|      3|      3|\n",
      "|        Social|      4|      4|\n",
      "+--------------+-------+-------+\n",
      "\n",
      "UPDATE public.dim_eixos SET \"descricao\" = %s, \"nk_eixo\" = %s WHERE \"nk_eixo\" = %s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m df_eixos\u001b[38;5;241m.\u001b[39mselect(columns \u001b[38;5;241m+\u001b[39m key_columns)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(create_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic.dim_eixos\u001b[39m\u001b[38;5;124m'\u001b[39m, columns, key_columns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUPDATE\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey_columns\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "def create_sql(table_name, columns, key_columns, type):\n",
    "    sql = ''\n",
    "    if type in ('DELETE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        sql = f'DELETE FROM {table_name} WHERE {columns_comparisson}'\n",
    "    elif type in ('INSERT'):\n",
    "        columns_str = list(map(lambda x: '\"' + x + '\"',columns))\n",
    "        columns_str = ', '.join(columns_str)\n",
    "        placeholders = ', '.join(['%s'] * len(columns))\n",
    "        sql = f'INSERT INTO {table_name}({columns_str}) VALUES({placeholders})'\n",
    "    elif type in ('UPDATE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        columns_set = [f'\"{column}\" = %s' for column in columns]\n",
    "        columns_set = ', '.join(columns_set)\n",
    "        sql = f'UPDATE {table_name} SET {columns_set} WHERE {columns_comparisson}'\n",
    "    return sql\n",
    "\n",
    "\n",
    "\n",
    "columns = ['descricao', 'nk_eixo']\n",
    "key_columns = ['nk_eixo']\n",
    "\n",
    "df_eixos.select(columns + key_columns).show()\n",
    "\n",
    "print(create_sql('public.dim_eixos', columns, key_columns, 'UPDATE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "df_eixos = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from stg_projeto_investimento_eixos\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  psycopg2 \n",
    "  \n",
    "#Method to create a connection object to the database. \n",
    "#It creates a pointer cursor to the database and returns it along with  Connection object \n",
    "def create_connection(properties, database_dw, host_dw, port_dw): \n",
    "      #Connect to the Postgresql database using the psycopg2 adapter. \n",
    "    #Pass your database name , username , password , hostname and port number \n",
    "    conn = psycopg2.connect(f\"dbname='{database_dw}' user='{properties['user']}' password='{properties['password']}' host='{host_dw}' port='{port_dw}'\") \n",
    "    #Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn,curr \n",
    "\n",
    "conn, curr = create_connection(properties, database_dw, host_dw, port_dw)\n",
    "\n",
    "deptpoints_update_query = \"\"\"INSERT INTO dim_eixos(nk_eixos,descricao) values(%s,%s)\"\"\"\n",
    "# Pass the new values and update query to the executemany() method of Cursor \n",
    "curr.executemany(deptpoints_update_query, data)\n",
    "conn.commit()\n",
    "conn.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-20.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-21.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-22.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-23.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-24.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-25.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-26.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-27.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-28.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-29.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-30.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-01.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-02.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-03.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-04.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-05.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-06.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-07.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-08.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-09.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-10.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-11.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-12.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-13.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-14.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-15.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-16.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-17.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-18.json']\n",
      "delete from stg_projeto_investimento_eixos stg\n",
      "                    using stg_projeto_investimento stg_project\n",
      "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ('RP','SCA');\n",
      "delete from stg_projeto_investimento_tomadores stg\n",
      "                    using stg_projeto_investimento stg_project\n",
      "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ('RP','SCA');\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, explode_outer\n",
    "\n",
    "def generate_dates(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "    return days_date\n",
    "\n",
    "today = datetime.now()\n",
    "dates = generate_dates(today, 30)\n",
    "\n",
    "df = []\n",
    "origins = []\n",
    "\n",
    "for date in dates:\n",
    "    origin_file = origin + f'/{date}.json'\n",
    "    if os.path.isfile(origin_file) == True:\n",
    "        origins.append(origin_file)\n",
    "\n",
    "print(origins)\n",
    "\n",
    "df_novo = spark.read.json(origins)\n",
    "df_novo = df_novo.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "\n",
    "sql_full = ''\n",
    "\n",
    "ufs = ['RP','SCA']\n",
    "\n",
    "tables = ['stg_projeto_investimento_eixos', 'stg_projeto_investimento_tomadores']\n",
    "for table in tables:\n",
    "    sql_full += f'''delete from {table} stg\n",
    "                    using stg_projeto_investimento stg_project\n",
    "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ({\",\".join(\"'\" + uf + \"'\"  for uf in ufs)});\\n'''\n",
    "print(sql_full)\n",
    "\n",
    "\n",
    "def generate_sql_deletion_ufs(tables, ufs):\n",
    "    sql_full = ''\n",
    "    for table in tables:\n",
    "        sql_full += f'''delete from {table} stg\n",
    "                        using stg_projeto_investimento stg_project\n",
    "                        where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.uf in ({\",\".join( \"'\" + uf + \"'\" for uf in ufs)});\\n'''\n",
    "    return sql_full\n",
    "\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(sql_full)\n",
    "curs.execute(f'delete from stg_projeto_investimento where uf in ({\",\".join(\"'\" + uf + \"'\"  for uf in ufs)});')\n",
    "\n",
    "#print( '(' + \",\".join(uf for uf in ufs) + ')')\n",
    "\n",
    "#df += df_novo\n",
    "\n",
    "#df_tomadores = df.withColumn(\"tomadores1\", explode(col(\"tomadores\"))).count()\n",
    "#df_tomadores\n",
    "len(origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "#df.write.jdbc(url=url, table='stg_execucao_financeira', mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Executions: 1\n",
      "Pages: 0\n",
      "N° Registers: 0\n",
      "Errors: 1\n",
      "Errors Consecutives: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 2\n",
      "Pages: 1\n",
      "N° Registers: 99\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 3\n",
      "Pages: 2\n",
      "N° Registers: 198\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 4\n",
      "Pages: 3\n",
      "N° Registers: 298\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 5\n",
      "Pages: 4\n",
      "N° Registers: 398\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 6\n",
      "Pages: 5\n",
      "N° Registers: 497\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 7\n",
      "Pages: 6\n",
      "N° Registers: 597\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 8\n",
      "Pages: 7\n",
      "N° Registers: 696\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 9\n",
      "Pages: 8\n",
      "N° Registers: 796\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 10\n",
      "Pages: 9\n",
      "N° Registers: 896\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 11\n",
      "Pages: 10\n",
      "N° Registers: 994\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 12\n",
      "Pages: 11\n",
      "N° Registers: 1094\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 13\n",
      "Pages: 12\n",
      "N° Registers: 1193\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 14\n",
      "Pages: 13\n",
      "N° Registers: 1293\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 15\n",
      "Pages: 14\n",
      "N° Registers: 1392\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 16\n",
      "Pages: 15\n",
      "N° Registers: 1492\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 17\n",
      "Pages: 16\n",
      "N° Registers: 1590\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 18\n",
      "Pages: 17\n",
      "N° Registers: 1689\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 19\n",
      "Pages: 18\n",
      "N° Registers: 1780\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 20\n",
      "Pages: 19\n",
      "N° Registers: 1880\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Execution Finished with success!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def extract_data_api(url_base, endpoint, uf, dest_path, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    method = \"GET\"\n",
    "    dest_path_file = dest_path + '/' + str(uf) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        url = url_base + endpoint + '?' + 'pagina=' + str(page) + '&' + 'tamanhoDaPagina=' + str(page_size) + '&' + 'uf=' + str(uf)\n",
    "        response = requests.request(method, url)\n",
    "        if response.status_code == 200:\n",
    "            if(len(response.json()[\"content\"]) == 0):\n",
    "                success = True \n",
    "            else:\n",
    "                page += 1\n",
    "                errors_consecutives = 0\n",
    "                executions += 1\n",
    "                content_all += response.json()[\"content\"]\n",
    "                #with open(dest_path_file, 'w') as f:\n",
    "                #    json.dump(response.json()[\"content\"],f)\n",
    "        else:\n",
    "            errors_consecutives += 1\n",
    "            errors += 1\n",
    "            executions += 1\n",
    "            if response.status_code == 429:\n",
    "                time.sleep(1)\n",
    "        print(f'Status Code: {response.status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "page_size = 100\n",
    "url_base = \"https://api.obrasgov.gestao.gov.br\"\n",
    "endpoint = \"/obrasgov/api/projeto-investimento\"\n",
    "current_year = datetime.now().year\n",
    "initial_year = int(current_year)\n",
    "final_year = int(current_year)\n",
    "dest_path = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "errors_limit = - 1\n",
    "errors_consecutives_limit = 5\n",
    "executions_limit = -1\n",
    "\n",
    "extract_data_api(url_base, endpoint, 'PR', dest_path, page_size, errors_limit, errors_consecutives_limit, executions_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-11 10:22:48.977500\n",
      "2024-10-12 10:22:48.977500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "today = datetime.today()\n",
    "days_before = today - timedelta(days=2)\n",
    "\n",
    "\n",
    "date_generated = [days_before + timedelta(days=x) for x in range(0, (today-days_before).days)]\n",
    "\n",
    "for d in date_generated:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(url_base, endpoint, param, method, path_dest_env, fun_api, page_size, errors_limit, errors_consecutives_limit, executions_limit):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    dest_path = os.getenv(path_dest_env)\n",
    "    dest_path_file = dest_path + '/' + str(param) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        success, content,  page, errors_consecutives, errors, executions, status_code =  fun_api(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions)\n",
    "\n",
    "        if content is not None:\n",
    "            content_all.append(content)\n",
    "\n",
    "        print(f'Status Code: {status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "def extract_api_projeto_investimento_uf(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"uf\": str(param)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        if(len(response.json()[\"content\"]) == 0):\n",
    "            success = True\n",
    "            return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "        else:\n",
    "            page += 1\n",
    "            errors_consecutives = 0\n",
    "            executions += 1\n",
    "            content = response.json()[\"content\"]\n",
    "            return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "\n",
    "@task()\n",
    "def extract_data_api_project(url_base, endpoint, date, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    method = \"GET\"\n",
    "    path_dest_env = \"PATH_DEST_PROJETO_INVESTIMENTO\"\n",
    "    extract_api(url_base, endpoint, date, method, path_dest_env, extract_api_projeto_investimento_uf, page_size, errors_limit, errors_consecutives_limit, executions_limit)\n",
    "\n",
    "def extract_api_execucao_financeira_year(url_base, endpoint, year, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"anoFinal\": str(year), \"anoInicial\": str(year)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    success = False\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        page += 1\n",
    "        errors_consecutives = 0\n",
    "        executions += 1\n",
    "        content = response.json()[\"content\"]\n",
    "        return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    elif response.status_code == 404:\n",
    "        success = True\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code \n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13\n",
      "2024-09-14\n",
      "2024-09-15\n",
      "2024-09-16\n",
      "2024-09-17\n",
      "2024-09-18\n",
      "2024-09-19\n",
      "2024-09-20\n",
      "2024-09-21\n",
      "2024-09-22\n",
      "2024-09-23\n",
      "2024-09-24\n",
      "2024-09-25\n",
      "2024-09-26\n",
      "2024-09-27\n",
      "2024-09-28\n",
      "2024-09-29\n",
      "2024-09-30\n",
      "2024-10-01\n",
      "2024-10-02\n",
      "2024-10-03\n",
      "2024-10-04\n",
      "2024-10-05\n",
      "2024-10-06\n",
      "2024-10-07\n",
      "2024-10-08\n",
      "2024-10-09\n",
      "2024-10-10\n",
      "2024-10-11\n",
      "2024-10-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2024-09-13',\n",
       " '2024-09-14',\n",
       " '2024-09-15',\n",
       " '2024-09-16',\n",
       " '2024-09-17',\n",
       " '2024-09-18',\n",
       " '2024-09-19',\n",
       " '2024-09-20',\n",
       " '2024-09-21',\n",
       " '2024-09-22',\n",
       " '2024-09-23',\n",
       " '2024-09-24',\n",
       " '2024-09-25',\n",
       " '2024-09-26',\n",
       " '2024-09-27',\n",
       " '2024-09-28',\n",
       " '2024-09-29',\n",
       " '2024-09-30',\n",
       " '2024-10-01',\n",
       " '2024-10-02',\n",
       " '2024-10-03',\n",
       " '2024-10-04',\n",
       " '2024-10-05',\n",
       " '2024-10-06',\n",
       " '2024-10-07',\n",
       " '2024-10-08',\n",
       " '2024-10-09',\n",
       " '2024-10-10',\n",
       " '2024-10-11',\n",
       " '2024-10-12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dates_reprocessing(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "        print(d.strftime('%Y-%m-%d'))\n",
    "    return days_date\n",
    "\n",
    "generate_dates_reprocessing(today, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
