{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 09:27:31 WARN Utils: Your hostname, Ubuntu-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/04/05 09:27:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 09:27:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Extraction_Data\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_delete_duplicates(table_name, key_columns):\n",
    "    select_key_columns = ' , '.join(key_columns)\n",
    "    comparison_keys_columns = [f'a.{key_column} = b.{key_column}' for key_column in key_columns]\n",
    "    where_key_columns = '\\nAND '.join(comparison_keys_columns)\n",
    "    sql = f'''DELETE FROM {table_name} a USING (\n",
    "                SELECT MIN(ctid) as ctid, {select_key_columns}\n",
    "                FROM {table_name} \n",
    "                GROUP BY ({select_key_columns}) HAVING COUNT(*) > 1\n",
    "            ) b\n",
    "            WHERE {where_key_columns}\n",
    "            AND a.ctid <> b.ctid'''\n",
    "    return sql\n",
    "\n",
    "\n",
    "def delete_duplicates(table_name, key_columns, connection_properties):\n",
    "    conn, curr = create_connection(connection_properties)\n",
    "    sql_delete = sql_delete_duplicates(table_name, key_columns)\n",
    "    curr.execute(sql_delete)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def delete_duplicates_stg():\n",
    "    connection_properties = {}\n",
    "    tables_key_columns = [('stg_projeto_investimento_eixos', ['idunico','id']),\n",
    "                          ('stg_projeto_investimento_executores', ['idunico', 'codigo']),\n",
    "                          ('stg_projeto_investimento_fontes_de_recurso', ['idunico', 'origem', 'valorinvestimentoprevisto']),\n",
    "                          ('stg_projeto_investimento_repassadores', ['idunico', 'codigo']),\n",
    "                          ('stg_projeto_investimento_sub_tipos', ['idunico', 'id']),\n",
    "                          ('stg_projeto_investimento_tipos', ['idunico', 'id']),\n",
    "                          ('stg_projeto_investimento_tomadores', ['idunico', 'codigo']),\n",
    "                          ('stg_projeto_investimento', ['idunico'])]\n",
    "    for table_name, key_columns in tables_key_columns:\n",
    "        print(f'Deleting duplicates on table {table_name}...')\n",
    "        delete_duplicates(table_name, key_columns, connection_properties)\n",
    "\n",
    "\n",
    "#delete_duplicates_stg()\n",
    "#print(sql_delete_duplicates('public.stg_projeto_investimento_eixos', ['idunico','id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 09:27:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "connection_properties = {'db_name':database_dw, 'user':user_dw, 'password':password_dw, 'host':host_dw, 'port':port_dw}\n",
    "\n",
    "\n",
    "def create_connection(connection_properties): \n",
    "    #Connect to the Postgresql database using the psycopg2 adapter. \n",
    "    #Pass your database name , username , password , hostname and port number \n",
    "    conn = psycopg2.connect(f\"dbname='{connection_properties['db_name']}' user='{connection_properties['user']}' password='{connection_properties['password']}'\\\n",
    "                            host='{connection_properties['host']}' port='{connection_properties['port']}'\") \n",
    "    #Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn,curr \n",
    "\n",
    "def delete_duplicates_distinct_years(table_name, years, connection_properties):\n",
    "    conn, curr = create_connection(connection_properties)\n",
    "\n",
    "    curr.execute(f'''create table IF NOT EXISTS {table_name}_temp (like {table_name})''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''INSERT INTO {table_name}_temp SELECT DISTINCT * FROM {table_name} where cast(\"nomeArquivo\" as integer) in ({','.join(years)})''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''delete from {table_name} where cast(\"nomeArquivo\" as integer) in ({','.join(years)})''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''insert into {table_name} select * from {table_name}_temp''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''drop table {table_name}_temp''')\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "def delete_duplicates_distinct(table_name, connection_properties):\n",
    "    conn, curr = create_connection(connection_properties)\n",
    "\n",
    "    curr.execute(f'''create table IF NOT EXISTS {table_name}_temp (like {table_name})''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''INSERT INTO {table_name}_temp SELECT DISTINCT * FROM {table_name}''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''delete from {table_name}''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''insert into {table_name} select * from {table_name}_temp''')\n",
    "    conn.commit()\n",
    "\n",
    "    curr.execute(f'''drop table {table_name}_temp''')\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "#delete_duplicates_distinct('stg_execucao_financeira', connection_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datetime.now().year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = 2023\n",
    "ano_final = 2024\n",
    "for ano in range(int(ano_inicial), int(ano_final) + 1):\n",
    "    print(ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = current_year\n",
    "ano_final = current_year\n",
    "\n",
    "dif_anos = (ano_final - ano_inicial) + 1\n",
    "for i in range(dif_anos):\n",
    "    print(ano_inicial + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/tmp/ipykernel_172561/2246629140.py:16: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  df = df.withColumn('teste',regexp_replace(input_file_name(), '.*\\/|\\.json$', ''))\n",
      "25/04/05 09:28:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+-----------------+------------------+-------------------+------------+----------------------------------+------------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-----------+----------------+--------------------+--------------------+--------------+--------------------+----------------------+--------------------+------------------+--------------------+----------+--------------------+--------------------+--------------------+---+-----------+-----+\n",
      "|       cep|dataCadastro|dataFinalEfetiva|dataFinalPrevista|dataInicialEfetiva|dataInicialPrevista|dataSituacao|descPlanoNacionalPoliticaVinculado|descPopulacaoBeneficiada|           descricao|               eixos|            endereco|    especie|          executores|     fontesDeRecurso|        funcaoSocial|          geometrias|    idUnico|isModeladaPorBim|          metaGlobal|            natureza|naturezaOutras|                nome|observacoesPertinentes|populacaoBeneficiada|qdtEmpregosGerados|        repassadores|  situacao|            subTipos|               tipos|           tomadores| uf|nomeArquivo|teste|\n",
      "+----------+------------+----------------+-----------------+------------------+-------------------+------------+----------------------------------+------------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-----------+----------------+--------------------+--------------------+--------------+--------------------+----------------------+--------------------+------------------+--------------------+----------+--------------------+--------------------+--------------------+---+-----------+-----+\n",
      "|      NULL|  2022-09-20|            NULL|       2025-12-31|              NULL|         2021-12-31|  2022-09-20|                              NULL|    Estrada com trafe...|Adequação de Estr...|       [{Social, 4}]|                NULL|Recuperação|[{75771279000106,...|[{Federal, 243633...|Atendimento aos p...|[{86820-000, 2024...| 7256.41-14|           false|Adequação de Estr...|                Obra|          NULL|Adequaçao de Estr...|                  NULL|                 200|                15|[{308799, MINISTÉ...|Cadastrada|[{Corredor com ou...|[{Desenvolvimento...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-03-23|            NULL|       2023-06-29|              NULL|         2020-06-29|  2023-03-23|                              NULL|                    NULL|Reforma do Centro...|       [{Social, 4}]|                NULL|    Reforma|[{76417005000186,...|[{Federal, 241275...|O Centro de Juven...|[{80530-908, 2023...|20700.41-14|           false|Reforma do Centro...|                Obra|          NULL|Reforma do CEJUV ...|                  NULL|                NULL|              NULL|[{51000, MINISTER...|Cadastrada|[{Centro Olímpico...|  [{Esporte, 38, 4}]|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2022-09-27|            NULL|       2025-04-30|              NULL|         2021-12-31|  2022-09-27|                              NULL|    População residen...|Pavimentação de V...|    [{Econômico, 3}]|                NULL| Construção|[{76965789000187,...|[{Federal, 101374...|Toda população do...|[{86420-000, 2024...| 8335.41-80|           false|Pavimentação de V...|                Obra|          NULL|Pavimentação de V...|                  NULL|                 260|                20|[{308798, MINISTÉ...|Cadastrada|[{Urbanização, 75...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-04-12|            NULL|       2025-04-10|              NULL|         2023-04-10|  2023-04-12|              POLÍTICA NACIONAL...|                    NULL|CONSTRUÇÃO DE CEN...|[{Administrativo,...|                NULL| Construção|[{95642286000115,...|[{Federal, 318750...|Familias que já s...|[{86755-000, 2023...|21112.41-34|           false|CONSTRUÇÃO DE CEN...|                Obra|          NULL|CONSTRUÇÃO DE CEN...|                  NULL|                NULL|              NULL|[{235874, MINISTÉ...|Cadastrada|[{Obras em Imóvei...|[{Assistência Soc...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-08-25|            NULL|       2025-09-30|              NULL|         2020-12-31|  2023-08-25|                              NULL|    Bairro Britador e...|       PAVIMENTAÇÃO.|    [{Econômico, 3}]|                NULL| Construção|[{76966845000106,...|[{Federal, 997228...|Os produtores ben...|[{86455-000, 2024...|24720.41-75|           false|       PAVIMENTAÇÃO.|Projeto de Invest...|          NULL|       PAVIMENTAÇAO |                  NULL|                 150|                10|[{14, MINISTÉRIO ...|Cadastrada|[{Pavimentação, 5...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2024-10-04|            NULL|       2029-10-25|              NULL|         2024-10-25|  2024-10-04|              Plano Nacional de...|                    NULL|Implantação do Si...|       [{Social, 4}]|                NULL| Construção|[{75828418000190,...|[{Federal, 3.0663...|2.400 famílias co...|[{86310-000, 2024...|43696.41-77|           false|Implantação do Si...|                Obra|          NULL|Implantação do Si...|                  NULL|                7200|              NULL|[{308798, MINISTÉ...|Cadastrada|[{Saneamento, 38,...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-08-28|            NULL|       2024-06-12|              NULL|         2020-06-12|  2023-08-28|                              NULL|    Toda a População ...|Pavimentação asfá...|       [{Social, 4}]|                NULL|Recuperação|[{76022490000199,...|[{Federal, 527852...|500 famílias do m...|[{83350-000, 2023...|24726.41-03|           false|Pavimentação asfá...|                Obra|          NULL|Pavimentação Asfá...|                  NULL|               18309|                12|[{308798, MINISTÉ...|Cadastrada|[{Urbanização, 75...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2022-12-29|            NULL|       2025-12-30|              NULL|         2022-12-31|  2022-12-29|                              NULL|                    NULL|Execução de Pavim...|       [{Social, 4}]|                NULL|Recuperação|[{78198975000163,...|[{Federal, 462787...|A população do Mu...|[{87360-000, 2024...|16745.41-40|            true|Execução de Pavim...|                Obra|          NULL|EXECUÇÃO DE RECAP...|                  NULL|                NULL|              NULL|[{308798, MINISTÉ...|Cadastrada|[{Urbanização, 75...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|83.203-190|  2023-03-24|            NULL|       2023-07-31|              NULL|         2023-03-24|  2023-03-24|                              NULL|                    NULL|Manutenção do tel...|[{Administrativo,...|Rua Benjamin Cons...|    Reforma|[{185, COMANDO DA...|[{Federal, 17000.0}]|Atendimento Médic...|[{83203-190, 2023...|20715.41-53|           false|Não comprometer o...|                Obra|          NULL|Telhado e Calha d...|                  NULL|                NULL|              NULL|[{41066, MINISTÉR...|Cadastrada|[{Obras em Imóvei...|[{Administrativo,...|[{185, COMANDO DA...| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-04-12|            NULL|       2023-11-30|              NULL|         2019-12-31|  2023-04-12|                              NULL|    toda população di...|Pavimentação das ...|       [{Social, 4}]|                NULL| Construção|[{75743567000157,...|[{Federal, 894545...|População do Bair...|[{86460-000, 2023...|21153.41-40|           false|Pavimentação das ...|Projeto de Invest...|          NULL|Pavimentação das ...|                  NULL|                7360|                 5|[{308799, MINISTÉ...|Cadastrada|[{Urbanização, 75...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|80.060-190|  2021-10-07|            NULL|       2022-05-18|              NULL|         2021-10-18|  2021-10-07|                              NULL|                    NULL|Fornecimento e in...|[{Administrativo,...|Rua General Carne...| Construção|[{394452003110, 1...|[{Federal, 132077...|Substituição do a...|[{NULL, 2021-10-0...| 4086.41-00|            NULL|Redução de custos...|                Obra|              |Execução de const...|                  NULL|                NULL|              NULL|[{117267, EMPRESA...|Cadastrada|[{Instituições Ho...|     [{Saúde, 4, 1}]|[{15126437002430,...| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-08-28|            NULL|       2024-12-31|              NULL|         2023-12-31|  2023-08-28|                              NULL|                    NULL|Pavimentação de V...|    [{Econômico, 3}]|                NULL| Construção|[{80881915000192,...|[{Federal, 961019...|Através da presen...|[{85826-000, 2023...|24780.41-59|           false|Pavimentação de V...|                Obra|          NULL|       Jardim Itália|                  NULL|                NULL|              NULL|[{308798, MINISTÉ...|Cadastrada|[{Pavimentação, 5...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-04-12|            NULL|       2025-11-30|              NULL|         2023-04-06|  2023-04-12|                              NULL|                    NULL|Ampliação e Refor...|       [{Social, 4}]|                NULL|  Ampliação|[{76002641000147,...|[{Federal, 400000...|Pessoas entre 0 e...|[{83880-000, 2023...|21184.41-98|           false|Ampliação e Refor...|                Obra|          NULL|Ampliação e Refor...|                  NULL|                NULL|              NULL|[{235874, MINISTÉ...|Cadastrada|[{Unidade de Acol...|[{Assistência Soc...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-12-06|            NULL|       2026-12-19|              NULL|         2023-12-19|  2023-12-06|                              NULL|                    NULL|Pavimentação de E...|    [{Econômico, 3}]|                NULL| Construção|[{76995430000152,...|[{Federal, 299000...|O público alvo da...|[{85580-000, 2024...|29291.41-17|           false|Pavimentação de E...|                Obra|          NULL|Pavimentação Asfá...|                  NULL|                NULL|              NULL|[{308799, MINISTÉ...|Cadastrada|[{Estradas Vicina...|[{Meio Ambiente, ...|                  []| PR|       f/PR|   PR|\n",
      "|87.360-000|  2021-12-09|            NULL|       2021-10-25|              NULL|         2021-01-15|  2021-12-09|                              NULL|                    NULL|Cobertura da quad...|[{Administrativo,...|Rodovia Luiz Dech...| Construção|[{26432, INST. FE...|[{Federal, 432209...|Quadra para práti...|[{NULL, 2021-12-0...| 5142.41-28|            NULL|Mais qualidade na...|                Obra|              |Cobertura da quad...|                  NULL|                NULL|              NULL|[{244, MINISTÉRIO...|Cadastrada| [{Educação, 84, 8}]|  [{Educação, 8, 1}]|                  []| PR|       f/PR|   PR|\n",
      "|80.230-150|  2023-04-14|            NULL|       2023-12-04|              NULL|         2023-07-03|  2023-04-14|                              NULL|                    NULL|Reforma da estrut...|[{Administrativo,...|Rua João Negrão, ...|    Reforma|[{26432, INST. FE...|[{Federal, 500000...|Reforma da estrut...|[{80230-150, 2023...|21288.41-81|           false|Obra de reforma d...|                Obra|          NULL|Campus Curitiba -...|                  NULL|                NULL|              NULL|[{244, MINISTÉRIO...|Cadastrada|[{Instituições Fe...|  [{Educação, 8, 1}]|[{26432, INST. FE...| PR|       f/PR|   PR|\n",
      "|85.760-000|  2023-04-14|            NULL|       2023-12-01|              NULL|         2023-08-01|  2023-04-14|                              NULL|                    NULL|Construção de ves...|[{Administrativo,...|Rua Cariris, 750 ...| Construção|[{26432, INST. FE...|[{Federal, 900000...|Construção de ves...|[{85760-000, 2023...|21289.41-38|           false|Construção de ves...|                Obra|          NULL|Campus Capanema -...|                  NULL|                NULL|              NULL|[{244, MINISTÉRIO...|Cadastrada|[{Instituições Fe...|  [{Educação, 8, 1}]|[{26432, INST. FE...| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-01-06|            NULL|       2023-11-15|              NULL|         2022-07-08|  2023-01-06|                              NULL|                    NULL|em decorrência do...|       [{Social, 4}]|                NULL|Recuperação|[{75132860000188,...|[{Federal, 118000...|Restabelecer aces...|[{70067-901, 2023...|19005.41-20|           false|em decorrência do...|                Obra|          NULL|reconstrução do p...|                  NULL|                NULL|              NULL|[{308799, MINISTÉ...|Cadastrada|[{Prevenção a Des...|[{Defesa Civil, 4...|[{75132860000188,...| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-04-13|            NULL|       2024-11-30|              NULL|         2021-11-05|  2023-04-13|                              NULL|    População do bair...|Adequação de estr...|    [{Econômico, 3}]|                NULL| Construção|[{76167717000194,...|[{Federal, 334584...|Com a readequação...|[{84925-000, 2023...|21227.41-41|           false|Adequação de estr...|                Obra|          NULL|Pavimentação Asfá...|                  NULL|                1500|                 5|[{14, MINISTÉRIO ...|Cadastrada|[{Pavimentação, 5...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "|      NULL|  2023-01-09|            NULL|       2023-12-30|              NULL|         2019-12-17|  2023-01-09|                              NULL|                    NULL|Revitalização da ...|    [{Econômico, 3}]|                NULL|    Reforma|[{78198975000163,...|[{Federal, 114683...|A população do mu...|[{87360-000, 2023...|19022.41-96|            true|Revitalização da ...|Projeto de Invest...|          NULL|REVITALIZAÇÃO DA ...|                  NULL|                NULL|              NULL|[{72084, MINISTÉR...|Cadastrada|[{Urbanização, 75...|[{Infraestrutura ...|                  []| PR|       f/PR|   PR|\n",
      "+----------+------------+----------------+-----------------+------------------+-------------------+------------+----------------------------------+------------------------+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-----------+----------------+--------------------+--------------------+--------------+--------------------+----------------------+--------------------+------------------+--------------------+----------+--------------------+--------------------+--------------------+---+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring, regexp_replace\n",
    "\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/uf'\n",
    "process_all = True\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/' + str(ano_inicial)+ '.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "    df = df.withColumn('teste',regexp_replace(input_file_name(), '.*\\/|\\.json$', ''))\n",
    "\n",
    "df.show()\n",
    "#print(df.show())\n",
    "#print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cep: string (nullable = true)\n",
      " |-- dataCadastro: date (nullable = true)\n",
      " |-- dataFinalEfetiva: string (nullable = true)\n",
      " |-- dataFinalPrevista: string (nullable = true)\n",
      " |-- dataInicialEfetiva: string (nullable = true)\n",
      " |-- dataInicialPrevista: string (nullable = true)\n",
      " |-- dataSituacao: string (nullable = true)\n",
      " |-- descPlanoNacionalPoliticaVinculado: string (nullable = true)\n",
      " |-- descPopulacaoBeneficiada: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      " |-- eixos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |-- endereco: string (nullable = true)\n",
      " |-- especie: string (nullable = true)\n",
      " |-- executores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- fontesDeRecurso: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- origem: string (nullable = true)\n",
      " |    |    |-- valorInvestimentoPrevisto: double (nullable = true)\n",
      " |-- funcaoSocial: string (nullable = true)\n",
      " |-- geometrias: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- cepAreaExecutora: string (nullable = true)\n",
      " |    |    |-- dataCriacao: string (nullable = true)\n",
      " |    |    |-- dataMetadado: string (nullable = true)\n",
      " |    |    |-- datum: string (nullable = true)\n",
      " |    |    |-- enderecoAreaExecutora: string (nullable = true)\n",
      " |    |    |-- geometria: string (nullable = true)\n",
      " |    |    |-- infoAdicionais: string (nullable = true)\n",
      " |    |    |-- nomeAreaExecutora: string (nullable = true)\n",
      " |    |    |-- origem: string (nullable = true)\n",
      " |    |    |-- paisAreaExecutora: string (nullable = true)\n",
      " |-- idUnico: string (nullable = true)\n",
      " |-- isModeladaPorBim: boolean (nullable = true)\n",
      " |-- metaGlobal: string (nullable = true)\n",
      " |-- natureza: string (nullable = true)\n",
      " |-- naturezaOutras: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- observacoesPertinentes: string (nullable = true)\n",
      " |-- populacaoBeneficiada: string (nullable = true)\n",
      " |-- qdtEmpregosGerados: string (nullable = true)\n",
      " |-- repassadores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- situacao: string (nullable = true)\n",
      " |-- subTipos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- idTipo: long (nullable = true)\n",
      " |-- tipos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descricao: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- idEixo: long (nullable = true)\n",
      " |-- tomadores: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- codigo: long (nullable = true)\n",
      " |    |    |-- nome: string (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring, explode, col, to_date, length\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date'\n",
    "process_all = False\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/2024-09-18.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "list_drop = ['tomadores','executores', 'teste']\n",
    "\n",
    "df_eixos = df.drop()\n",
    "df_eixos = df_eixos.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "drop_columns = []\n",
    "\n",
    "for column in df.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "\n",
    "drop_columns\n",
    "\n",
    "#df_eixos = df.drop(*drop_columns)\n",
    "df_eixos.printSchema()\n",
    "#df.schema[5].dataType\n",
    "\n",
    "\n",
    "\n",
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')\n",
    "\n",
    "#print(df.show())\n",
    "#print(df.printSchema())\n",
    "#df.createOrReplaceTempView(\"eixos\")\n",
    "\n",
    "#results = spark.sql(\"select idUnico, eixos.eixos.* from eixos\")\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idUnico 11\n",
      "cepAreaExecutora 9\n",
      "datum 11\n",
      "enderecoAreaExecutora 51\n",
      "geometria 42\n",
      "infoAdicionais 92\n",
      "nomeAreaExecutora 60\n",
      "origem 11\n",
      "paisAreaExecutora 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf_projeto_investimento = df.drop(*drop_columns)\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialPrevista\", to_date(col(\"dataInicialPrevista\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalPrevista\", to_date(col(\"dataFinalPrevista\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialEfetiva\", to_date(col(\"dataInicialEfetiva\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalEfetiva\", to_date(col(\"dataFinalEfetiva\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento = df_projeto_investimento.withColumn(\"dataSituacao\", to_date(col(\"dataSituacao\"), \"yyyy-MM-dd\"))\\ndf_projeto_investimento.write.jdbc(url=url, table=\\'stg_projeto_investimento\\', mode=mode, properties=properties)\\n\\ntoday = datetime.now()\\ndate_before = today - timedelta(days=30)\\n\\nmode = \\'append\\'\\nurl = f\\'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}\\'\\nproperties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\\nconn = jaydebeapi.connect(driver, url, [user_dw, password_dw], \\'/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\\')\\ncurs = conn.cursor()\\ncurs.execute(f\\'\\'\\'delete from stg_projeto_investimento_eixos         where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\n                 delete from stg_projeto_investimento               where \"datacadastro\" between \\'{date_before}\\' and \\'{today}\\';\\'\\'\\')\\n\\n#mode = \\'append\\'\\n#properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\\n#df_eixos.write.jdbc(url=url, table=\\'stg_projeto_investimento_fontes_de_recurso\\', mode=mode, properties=properties)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "mode = 'append'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "\n",
    "df_geometria = df.withColumn(\"geometrias1\", explode(\"geometrias\")).select(\"idUnico\",\"dataCadastro\",\"geometrias1.*\")\n",
    "df_geometria = df_geometria.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_geometria = df_geometria.withColumn(\"dataCriacao\", to_date(col(\"dataCriacao\"), \"yyyy-MM-dd\"))\n",
    "df_geometria = df_geometria.withColumn(\"dataMetadado\", to_date(col(\"dataMetadado\"), \"yyyy-MM-dd\"))\n",
    "df_geometria.write.jdbc(url=url, table='stg_projeto_investimento_geometria', mode=mode, properties=properties)\n",
    "\n",
    "drop_columns = []\n",
    "for column in df_geometria.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "    if type(column.dataType) == StringType:\n",
    "        print(column.name,df_geometria.withColumn(\"len_col\",length(col(column.name))).groupby().max(\"len_col\").head()[0])\n",
    "\n",
    "\"\"\"\n",
    "df_projeto_investimento = df.drop(*drop_columns)\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialPrevista\", to_date(col(\"dataInicialPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalPrevista\", to_date(col(\"dataFinalPrevista\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataInicialEfetiva\", to_date(col(\"dataInicialEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataFinalEfetiva\", to_date(col(\"dataFinalEfetiva\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento = df_projeto_investimento.withColumn(\"dataSituacao\", to_date(col(\"dataSituacao\"), \"yyyy-MM-dd\"))\n",
    "df_projeto_investimento.write.jdbc(url=url, table='stg_projeto_investimento', mode=mode, properties=properties)\n",
    "\n",
    "today = datetime.now()\n",
    "date_before = today - timedelta(days=30)\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(f'''delete from stg_projeto_investimento_eixos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento               where \"datacadastro\" between '{date_before}' and '{today}';''')\n",
    "\n",
    "#mode = 'append'\n",
    "#properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#df_eixos.write.jdbc(url=url, table='stg_projeto_investimento_fontes_de_recurso', mode=mode, properties=properties)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nk_eixo: string (nullable = true)\n",
      " |-- descricao: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cast\n",
    "\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "\n",
    "df_eixos = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select distinct cast(id as varchar) as nk_eixo, descricao from stg_projeto_investimento_eixos\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load()\n",
    "\n",
    "df_eixos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2', 'Militar'),\n",
       " ('1', 'Administrativo'),\n",
       " ('3', 'Econômico'),\n",
       " ('4', 'Social')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df_eixos.rdd\n",
    "data = rdd.map(tuple)\n",
    "\n",
    "data = data.collect()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Militar32:>                                                         (0 + 1) / 1]\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|nk_eixo|        descricao|\n",
      "+-------+-----------------+\n",
      "|      1|Administrativoabc|\n",
      "|      3|     Econômicoabc|\n",
      "|      4|           Social|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "from pyspark.sql.functions import udf \n",
    "\n",
    "@udf(StringType())\n",
    "def concat_names(column):\n",
    "    print(column)\n",
    "    if column not in ('Militar', 'Social'):\n",
    "        abc = column + 'abc'\n",
    "        return abc\n",
    "    return column\n",
    "\n",
    "df_eixos_2 = df_eixos\n",
    "df_eixos_2 = df_eixos_2.withColumn('descricao', concat_names('descricao'))\n",
    "df_eixos_2 = df_eixos_2.filter(\"descricao != 'Militar'\")\n",
    "df_eixos_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Militar\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n",
      "Administrativo\n",
      "Econômico\n",
      "Social\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|nk_eixo|        descricao|\n",
      "+-------+-----------------+\n",
      "|      1|Administrativoabc|\n",
      "|      3|     Econômicoabc|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "from functools import reduce\n",
    "\n",
    "def values_to_update(df_old, df_new, key_columns, compare_columns):\n",
    "    \"\"\"\n",
    "    Compared two dataframes and find the rows to update.\n",
    "\n",
    "    :param df_old: Old DataFrame\n",
    "    :param df_new: New DataFrame\n",
    "    :param key_columns: Join columns\n",
    "    :param compare_columns: Comparisson coluns\n",
    "    :return: Update DataFrame\n",
    "    \"\"\"\n",
    "    # Criar a condição de join\n",
    "    join_condition = [col(f\"new.{key}\") == col(f\"old.{key}\") for key in key_columns]\n",
    "    \n",
    "    # Realizar o join\n",
    "    joined_df = df_new.alias(\"new\").join(df_old.alias(\"old\"), on=join_condition, how=\"inner\")\n",
    "\n",
    "    # Filtrar as diferenças\n",
    "    # filter_condition = [\n",
    "    #    (col(f\"new.{column}\") != col(f\"old.{column}\")) for column in compare_columns\n",
    "    #]\n",
    "\n",
    "    filter_condition = [\n",
    "        f'not new.{column} <=> old.{column}' for column in compare_columns\n",
    "    ]\n",
    "\n",
    "    # Aplicar o filtro\n",
    "    #differences = joined_df.filter(reduce(lambda a, b: a | b, filter_condition))\n",
    "    differences = joined_df.filter(' or '.join(filter_condition))\n",
    "    \n",
    "    return differences.select(\"new.*\")\n",
    "\n",
    "def values_to_insert_delete(df1, df2, key_columns):\n",
    "    \"\"\"\n",
    "    Compared two dataframes and find the rows to update.\n",
    "\n",
    "    :param df1: Secondary DataFrame\n",
    "    :param df2: Main DataFrame\n",
    "    :param key_columns: Join columns\n",
    "    :return: Insert Or Delete DataFrame\n",
    "    \"\"\"\n",
    "    # Criar a condição de join\n",
    "    join_condition = [col(f\"new.{key}\") == col(f\"old.{key}\") for key in key_columns]\n",
    "    \n",
    "    # Realizar o join\n",
    "    joined_df = df2.alias(\"new\").join(df1.alias(\"old\"), on=join_condition, how=\"leftanti\")\n",
    "\n",
    "    return joined_df.select(\"new.*\")\n",
    "\n",
    "def values_to_delete(df1, df2, key_columns):\n",
    "    \"\"\"\n",
    "    Compara dois DataFrames do Spark com base em colunas chave e colunas a serem comparadas.\n",
    "\n",
    "    :param df1: Primeiro DataFrame\n",
    "    :param df2: Segundo DataFrame\n",
    "    :param key_columns: Lista de colunas chave para o join\n",
    "    :param compare_columns: Lista de colunas a serem comparadas\n",
    "    :return: DataFrame contendo as diferenças\n",
    "    \"\"\"\n",
    "    # Criar a condição de join\n",
    "    join_condition = [col(f\"new.{key}\") == col(f\"old.{key}\") for key in key_columns]\n",
    "    \n",
    "    # Realizar o join\n",
    "    joined_df = df1.alias(\"new\").join(df2.alias(\"old\"), on=join_condition, how=\"leftanti\")\n",
    "\n",
    "    return joined_df.select(\"new.*\")\n",
    "\n",
    "teste = values_to_update(df_eixos, df_eixos_2, ['nk_eixo'], ['descricao'])\n",
    "\n",
    "#teste = values_to_insert(df_eixos, df_eixos_2, ['nk_eixo'])\n",
    "\n",
    "#teste = values_to_delete(df_eixos, df_eixos_2, ['nk_eixo'])\n",
    " \n",
    "teste.show()\n",
    "\n",
    "#conditions_ = len([when(df_eixos[c]!=df_eixos_2[c], lit(c)).otherwise(\"\") for c in df_eixos.columns if c != 'nk_eixo']) == 0\n",
    "\n",
    "#df_eixos.join(df_eixos_2, ['nk_eixo'], how='outer').select(df_eixos.descricao, df_eixos_2.descricao).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|nk_eixo|     descricao|\n",
      "+-------+--------------+\n",
      "|      2|       Militar|\n",
      "|      1|Administrativo|\n",
      "|      3|     Econômico|\n",
      "+-------+--------------+\n",
      "\n",
      "None\n",
      "+-------+---------+\n",
      "|nk_eixo|descricao|\n",
      "+-------+---------+\n",
      "|      4|   Social|\n",
      "+-------+---------+\n",
      "\n",
      "None\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nk_eixo', 'descricao']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "bulk_size = 3\n",
    "split_w = ceil(df_eixos.count() / bulk_size)\n",
    "df_result = df_eixos\n",
    "for i in range(split_w):\n",
    "    df_eixos_limited = df_result.limit(bulk_size)\n",
    "    df_result = df_result.subtract(df_eixos_limited)\n",
    "    print(df_eixos_limited.show())\n",
    "\n",
    "print(split_w)\n",
    "\n",
    "df_eixos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  psycopg2 \n",
    "from math import ceil\n",
    "\n",
    "f\"dbname='{database_dw}' user='{properties['user']}' password='{properties['password']}' host='{host_dw}' port='{port_dw}'\"\n",
    "\n",
    "def create_sql(table_name, columns, key_columns, type):\n",
    "    sql = ''\n",
    "    if type in ('DELETE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        sql = f'DELETE FROM {table_name} WHERE {columns_comparisson}'\n",
    "    elif type in ('INSERT'):\n",
    "        columns_str = list(map(lambda x: '\"' + x + '\"',columns + key_columns))\n",
    "        columns_str = ', '.join(columns_str)\n",
    "        placeholders = ', '.join(['%s'] * len(columns + key_columns))\n",
    "        sql = f'INSERT INTO {table_name}({columns_str}) VALUES({placeholders})'\n",
    "    elif type in ('UPDATE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        columns_set = [f'\"{column}\" = %s' for column in columns]\n",
    "        columns_set = ', '.join(columns_set)\n",
    "        sql = f'UPDATE {table_name} SET {columns_set} WHERE {columns_comparisson}'\n",
    "\n",
    "    return sql\n",
    "\n",
    "def create_connection(connection_properties): \n",
    "    #Connect to the Postgresql database using the psycopg2 adapter. \n",
    "    #Pass your database name , username , password , hostname and port number \n",
    "    conn = psycopg2.connect(f\"dbname='{connection_properties['db_name']}' user='{connection_properties['user']}' password='{connection_properties['password']}'\\\n",
    "                            host='{connection_properties['host']}' port='{connection_properties['port']}'\") \n",
    "    #Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn,curr \n",
    "\n",
    "def bulk_values(table_name, df, bulk_size, connection_properties, type, columns, key_columns=[]):\n",
    "\n",
    "    if df.count() == 0:\n",
    "        return\n",
    "\n",
    "    splits = ceil(df.count() / bulk_size)\n",
    "    conn, curr = create_connection(connection_properties)\n",
    "    rdn_splits = [float(bulk_size)] * splits\n",
    "    splits_dfs = df.randomSplit(rdn_splits)\n",
    "\n",
    "\n",
    "    for split_index, split_df in enumerate(splits_dfs):\n",
    "\n",
    "        if split_df.count() == 0:\n",
    "            continue\n",
    "\n",
    "        query = create_sql(table_name, columns, key_columns, type)\n",
    "\n",
    "        if type in ('UPDATE', 'INSERT'):\n",
    "            split_df = split_df.select(columns + key_columns)\n",
    "        elif type in ('DELETE'):\n",
    "            split_df = split_df.select(key_columns)\n",
    "\n",
    "\n",
    "        rdd = split_df.rdd\n",
    "        data = rdd.map(tuple)\n",
    "        data = data.collect()\n",
    "\n",
    "\n",
    "        curr.executemany(query, data) \n",
    "        conn.commit() \n",
    "\n",
    "        print(f'Split: {split_index + 1}\\\\{splits}\\\n",
    "                Bulk Size: {split_df.count()}')\n",
    "        split_df.unpersist()\n",
    "\n",
    "    conn.close() \n",
    "\n",
    "df_execucao_fin = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from public.stg_projeto_investimento\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load()\n",
    "\n",
    "table_name = 'stg_projeto_investimento_teste'\n",
    "connection_properties = {'db_name':database_dw, 'user':user_dw, 'password':password_dw, 'host':host_dw, 'port':port_dw}\n",
    "\n",
    "#bulk_values(table_name, df_execucao_fin, 500, connection_properties, 'UPDATE' ,df_execucao_fin.columns, ['idunico'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting delete...\n",
      "Deleted 0 rows\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n"
     ]
    }
   ],
   "source": [
    "df_project_inv = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from public.stg_projeto_investimento\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load().cache()\n",
    "\n",
    "df_project_inv_teste = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from public.stg_projeto_investimento_teste\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load().cache()\n",
    "\n",
    "def crud_database(df_old, df_new, table_name, key_columns, connection_properties, bulk_size, insert = True, update = True, delete = False):\n",
    "    columns_comparisson = df_new.columns\n",
    "    columns_comparisson = [column for column in columns_comparisson if not column in key_columns] \n",
    "\n",
    "    if delete == True:\n",
    "        df_delete = values_to_insert_delete(df_new, df_old, key_columns)\n",
    "        print('Starting delete...')\n",
    "        bulk_values(table_name, df_delete, bulk_size, connection_properties, 'DELETE' ,columns_comparisson, key_columns)\n",
    "        print(f'Deleted {df_delete.count()} rows')\n",
    "        df_delete.unpersist()\n",
    "\n",
    "    if update == True:\n",
    "        df_update = values_to_update(df_old, df_new, key_columns, columns_comparisson)\n",
    "        print('Starting update...')\n",
    "        bulk_values(table_name, df_update, bulk_size, connection_properties, 'UPDATE' ,columns_comparisson, key_columns)\n",
    "        print(f'Updated {df_update.count()} rows')\n",
    "        df_update.unpersist()\n",
    "\n",
    "    if insert == True:\n",
    "        df_insert = values_to_insert_delete(df_old, df_new, key_columns)\n",
    "        print('Starting insert...')\n",
    "        bulk_values(table_name, df_insert, bulk_size, connection_properties, 'INSERT' ,columns_comparisson, key_columns)\n",
    "        print(f'Inserted {df_insert.count()} rows')\n",
    "        df_insert.unpersist()\n",
    "\n",
    "\n",
    "def crud_database_table(sql_old, sql_new, table_name, key_columns, connection_properties, bulk_size, insert = True, update = True, delete = False):\n",
    "\n",
    "    url = f'jdbc:postgresql://{connection_properties['host']}:{connection_properties['port']}/{connection_properties['db_name']}'\n",
    "\n",
    "    df_new = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"query\", sql_new)\\\n",
    "        .option(\"driver\", connection_properties['driver'])\\\n",
    "        .option(\"user\", connection_properties['user'])\\\n",
    "        .option(\"password\", connection_properties['password'])\\\n",
    "        .load().cache()\n",
    "\n",
    "    df_old = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"query\", sql_old)\\\n",
    "        .option(\"driver\", connection_properties['driver'])\\\n",
    "        .option(\"user\", connection_properties['user'])\\\n",
    "        .option(\"password\", connection_properties['password'])\\\n",
    "        .load().cache()\n",
    "    \n",
    "    crud_database(df_old, df_new, table_name, key_columns, connection_properties, bulk_size, insert, update, delete)\n",
    "\n",
    "    df_new.unpersist()\n",
    "    df_old.unpersist()\n",
    "\n",
    "\n",
    "connection_properties = {'db_name':database_dw, 'user':user_dw, 'password':password_dw, 'host':host_dw, 'port':port_dw, 'driver': \"org.postgresql.Driver\"}\n",
    "table_name = 'public.stg_projeto_investimento_teste'\n",
    "sql_new = 'select * from public.stg_projeto_investimento'\n",
    "sql_old = 'select * from public.stg_projeto_investimento_teste'\n",
    "key_columns = ['idunico']\n",
    "bulk_size = 500\n",
    "\n",
    "crud_database_table(sql_old, sql_new, table_name, key_columns, connection_properties, bulk_size, delete=True)\n",
    "\n",
    "\n",
    "#columns_comparisson = df_project_inv_teste.columns\n",
    "#columns_comparisson = [column for column in columns_comparisson if not column in key_columns] \n",
    "\n",
    "#crud_database(df_project_inv_teste, df_project_inv , table_name, key_columns, connection_properties, 500, delete=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#key_columns = ['idunico']\n",
    "#columns_comparisson = df_project_inv.columns\n",
    "\n",
    "#values_to_update(df_project_inv,df_project_inv_teste,key_columns,columns_comparisson).show()\n",
    "\n",
    "#df_project_inv_teste.filter('idunico <=> \"13414.41-30\"').select('qdtempregosgerados').show()\n",
    "\n",
    "#[column for column in columns_comparisson if not column in key_columns or key_columns.remove(column)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+\n",
      "|     descricao|nk_eixo|nk_eixo|\n",
      "+--------------+-------+-------+\n",
      "|       Militar|      2|      2|\n",
      "|Administrativo|      1|      1|\n",
      "|     Econômico|      3|      3|\n",
      "|        Social|      4|      4|\n",
      "+--------------+-------+-------+\n",
      "\n",
      "UPDATE public.dim_eixos SET \"descricao\" = %s, \"nk_eixo\" = %s WHERE \"nk_eixo\" = %s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m df_eixos\u001b[38;5;241m.\u001b[39mselect(columns \u001b[38;5;241m+\u001b[39m key_columns)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(create_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic.dim_eixos\u001b[39m\u001b[38;5;124m'\u001b[39m, columns, key_columns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUPDATE\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey_columns\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "def create_sql(table_name, columns, key_columns, type):\n",
    "    sql = ''\n",
    "    if type in ('DELETE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        sql = f'DELETE FROM {table_name} WHERE {columns_comparisson}'\n",
    "    elif type in ('INSERT'):\n",
    "        columns_str = list(map(lambda x: '\"' + x + '\"',columns))\n",
    "        columns_str = ', '.join(columns_str)\n",
    "        placeholders = ', '.join(['%s'] * len(columns))\n",
    "        sql = f'INSERT INTO {table_name}({columns_str}) VALUES({placeholders})'\n",
    "    elif type in ('UPDATE'):\n",
    "        columns_comparisson = [f'\"{column}\" = %s' for column in key_columns]\n",
    "        columns_comparisson = ' and '.join(columns_comparisson)\n",
    "        columns_set = [f'\"{column}\" = %s' for column in columns]\n",
    "        columns_set = ', '.join(columns_set)\n",
    "        sql = f'UPDATE {table_name} SET {columns_set} WHERE {columns_comparisson}'\n",
    "    return sql\n",
    "\n",
    "\n",
    "\n",
    "columns = ['descricao', 'nk_eixo']\n",
    "key_columns = ['nk_eixo']\n",
    "\n",
    "df_eixos.select(columns + key_columns).show()\n",
    "\n",
    "print(create_sql('public.dim_eixos', columns, key_columns, 'UPDATE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "df_eixos = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", url)\\\n",
    "    .option(\"query\", \"select * from stg_projeto_investimento_eixos\")\\\n",
    "    .option(\"driver\", properties[\"driver\"])\\\n",
    "    .option(\"user\", properties[\"user\"])\\\n",
    "    .option(\"password\", properties[\"password\"])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  psycopg2 \n",
    "  \n",
    "#Method to create a connection object to the database. \n",
    "#It creates a pointer cursor to the database and returns it along with  Connection object \n",
    "def create_connection(properties, database_dw, host_dw, port_dw): \n",
    "      #Connect to the Postgresql database using the psycopg2 adapter. \n",
    "    #Pass your database name , username , password , hostname and port number \n",
    "    conn = psycopg2.connect(f\"dbname='{database_dw}' user='{properties['user']}' password='{properties['password']}' host='{host_dw}' port='{port_dw}'\") \n",
    "    #Get the cursor object from the connection object \n",
    "    curr = conn.cursor() \n",
    "    return conn,curr \n",
    "\n",
    "conn, curr = create_connection(properties, database_dw, host_dw, port_dw)\n",
    "\n",
    "deptpoints_update_query = \"\"\"INSERT INTO dim_eixos(nk_eixos,descricao) values(%s,%s)\"\"\"\n",
    "# Pass the new values and update query to the executemany() method of Cursor \n",
    "curr.executemany(deptpoints_update_query, data)\n",
    "conn.commit()\n",
    "conn.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-20.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-21.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-22.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-23.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-24.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-25.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-26.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-27.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-28.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-29.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-09-30.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-01.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-02.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-03.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-04.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-05.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-06.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-07.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-08.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-09.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-10.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-11.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-12.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-13.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-14.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-15.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-16.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-17.json', '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date/2024-10-18.json']\n",
      "delete from stg_projeto_investimento_eixos stg\n",
      "                    using stg_projeto_investimento stg_project\n",
      "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ('RP','SCA');\n",
      "delete from stg_projeto_investimento_tomadores stg\n",
      "                    using stg_projeto_investimento stg_project\n",
      "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ('RP','SCA');\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, explode_outer\n",
    "\n",
    "def generate_dates(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "    return days_date\n",
    "\n",
    "today = datetime.now()\n",
    "dates = generate_dates(today, 30)\n",
    "\n",
    "df = []\n",
    "origins = []\n",
    "\n",
    "for date in dates:\n",
    "    origin_file = origin + f'/{date}.json'\n",
    "    if os.path.isfile(origin_file) == True:\n",
    "        origins.append(origin_file)\n",
    "\n",
    "print(origins)\n",
    "\n",
    "df_novo = spark.read.json(origins)\n",
    "df_novo = df_novo.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "\n",
    "sql_full = ''\n",
    "\n",
    "ufs = ['RP','SCA']\n",
    "\n",
    "tables = ['stg_projeto_investimento_eixos', 'stg_projeto_investimento_tomadores']\n",
    "for table in tables:\n",
    "    sql_full += f'''delete from {table} stg\n",
    "                    using stg_projeto_investimento stg_project\n",
    "                    where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.\"uf\" in ({\",\".join(\"'\" + uf + \"'\"  for uf in ufs)});\\n'''\n",
    "print(sql_full)\n",
    "\n",
    "\n",
    "def generate_sql_deletion_ufs(tables, ufs):\n",
    "    sql_full = ''\n",
    "    for table in tables:\n",
    "        sql_full += f'''delete from {table} stg\n",
    "                        using stg_projeto_investimento stg_project\n",
    "                        where stg_project.\"idunico\" = stg.\"idunico\" and stg_project.uf in ({\",\".join( \"'\" + uf + \"'\" for uf in ufs)});\\n'''\n",
    "    return sql_full\n",
    "\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(sql_full)\n",
    "curs.execute(f'delete from stg_projeto_investimento where uf in ({\",\".join(\"'\" + uf + \"'\"  for uf in ufs)});')\n",
    "\n",
    "#print( '(' + \",\".join(uf for uf in ufs) + ')')\n",
    "\n",
    "#df += df_novo\n",
    "\n",
    "#df_tomadores = df.withColumn(\"tomadores1\", explode(col(\"tomadores\"))).count()\n",
    "#df_tomadores\n",
    "len(origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "#df.write.jdbc(url=url, table='stg_execucao_financeira', mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Executions: 1\n",
      "Pages: 0\n",
      "N° Registers: 0\n",
      "Errors: 1\n",
      "Errors Consecutives: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 2\n",
      "Pages: 1\n",
      "N° Registers: 99\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 3\n",
      "Pages: 2\n",
      "N° Registers: 198\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 4\n",
      "Pages: 3\n",
      "N° Registers: 298\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 5\n",
      "Pages: 4\n",
      "N° Registers: 398\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 6\n",
      "Pages: 5\n",
      "N° Registers: 497\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 7\n",
      "Pages: 6\n",
      "N° Registers: 597\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 8\n",
      "Pages: 7\n",
      "N° Registers: 696\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 9\n",
      "Pages: 8\n",
      "N° Registers: 796\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 10\n",
      "Pages: 9\n",
      "N° Registers: 896\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 11\n",
      "Pages: 10\n",
      "N° Registers: 994\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 12\n",
      "Pages: 11\n",
      "N° Registers: 1094\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 13\n",
      "Pages: 12\n",
      "N° Registers: 1193\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 14\n",
      "Pages: 13\n",
      "N° Registers: 1293\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 15\n",
      "Pages: 14\n",
      "N° Registers: 1392\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 16\n",
      "Pages: 15\n",
      "N° Registers: 1492\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 17\n",
      "Pages: 16\n",
      "N° Registers: 1590\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 18\n",
      "Pages: 17\n",
      "N° Registers: 1689\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 19\n",
      "Pages: 18\n",
      "N° Registers: 1780\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 20\n",
      "Pages: 19\n",
      "N° Registers: 1880\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Execution Finished with success!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def extract_data_api(url_base, endpoint, uf, dest_path, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    method = \"GET\"\n",
    "    dest_path_file = dest_path + '/' + str(uf) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        url = url_base + endpoint + '?' + 'pagina=' + str(page) + '&' + 'tamanhoDaPagina=' + str(page_size) + '&' + 'uf=' + str(uf)\n",
    "        response = requests.request(method, url)\n",
    "        if response.status_code == 200:\n",
    "            if(len(response.json()[\"content\"]) == 0):\n",
    "                success = True \n",
    "            else:\n",
    "                page += 1\n",
    "                errors_consecutives = 0\n",
    "                executions += 1\n",
    "                content_all += response.json()[\"content\"]\n",
    "                #with open(dest_path_file, 'w') as f:\n",
    "                #    json.dump(response.json()[\"content\"],f)\n",
    "        else:\n",
    "            errors_consecutives += 1\n",
    "            errors += 1\n",
    "            executions += 1\n",
    "            if response.status_code == 429:\n",
    "                time.sleep(1)\n",
    "        print(f'Status Code: {response.status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "page_size = 100\n",
    "url_base = \"https://api.obrasgov.gestao.gov.br\"\n",
    "endpoint = \"/obrasgov/api/projeto-investimento\"\n",
    "current_year = datetime.now().year\n",
    "initial_year = int(current_year)\n",
    "final_year = int(current_year)\n",
    "dest_path = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "errors_limit = - 1\n",
    "errors_consecutives_limit = 5\n",
    "executions_limit = -1\n",
    "\n",
    "extract_data_api(url_base, endpoint, 'PR', dest_path, page_size, errors_limit, errors_consecutives_limit, executions_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-11 10:22:48.977500\n",
      "2024-10-12 10:22:48.977500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "today = datetime.today()\n",
    "days_before = today - timedelta(days=2)\n",
    "\n",
    "\n",
    "date_generated = [days_before + timedelta(days=x) for x in range(0, (today-days_before).days)]\n",
    "\n",
    "for d in date_generated:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(url_base, endpoint, param, method, path_dest_env, fun_api, page_size, errors_limit, errors_consecutives_limit, executions_limit):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    dest_path = os.getenv(path_dest_env)\n",
    "    dest_path_file = dest_path + '/' + str(param) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        success, content,  page, errors_consecutives, errors, executions, status_code =  fun_api(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions)\n",
    "\n",
    "        if content is not None:\n",
    "            content_all.append(content)\n",
    "\n",
    "        print(f'Status Code: {status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "def extract_api_projeto_investimento_uf(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"uf\": str(param)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        if(len(response.json()[\"content\"]) == 0):\n",
    "            success = True\n",
    "            return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "        else:\n",
    "            page += 1\n",
    "            errors_consecutives = 0\n",
    "            executions += 1\n",
    "            content = response.json()[\"content\"]\n",
    "            return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "\n",
    "@task()\n",
    "def extract_data_api_project(url_base, endpoint, date, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    method = \"GET\"\n",
    "    path_dest_env = \"PATH_DEST_PROJETO_INVESTIMENTO\"\n",
    "    extract_api(url_base, endpoint, date, method, path_dest_env, extract_api_projeto_investimento_uf, page_size, errors_limit, errors_consecutives_limit, executions_limit)\n",
    "\n",
    "def extract_api_execucao_financeira_year(url_base, endpoint, year, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"anoFinal\": str(year), \"anoInicial\": str(year)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    success = False\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        page += 1\n",
    "        errors_consecutives = 0\n",
    "        executions += 1\n",
    "        content = response.json()[\"content\"]\n",
    "        return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    elif response.status_code == 404:\n",
    "        success = True\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code \n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13\n",
      "2024-09-14\n",
      "2024-09-15\n",
      "2024-09-16\n",
      "2024-09-17\n",
      "2024-09-18\n",
      "2024-09-19\n",
      "2024-09-20\n",
      "2024-09-21\n",
      "2024-09-22\n",
      "2024-09-23\n",
      "2024-09-24\n",
      "2024-09-25\n",
      "2024-09-26\n",
      "2024-09-27\n",
      "2024-09-28\n",
      "2024-09-29\n",
      "2024-09-30\n",
      "2024-10-01\n",
      "2024-10-02\n",
      "2024-10-03\n",
      "2024-10-04\n",
      "2024-10-05\n",
      "2024-10-06\n",
      "2024-10-07\n",
      "2024-10-08\n",
      "2024-10-09\n",
      "2024-10-10\n",
      "2024-10-11\n",
      "2024-10-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2024-09-13',\n",
       " '2024-09-14',\n",
       " '2024-09-15',\n",
       " '2024-09-16',\n",
       " '2024-09-17',\n",
       " '2024-09-18',\n",
       " '2024-09-19',\n",
       " '2024-09-20',\n",
       " '2024-09-21',\n",
       " '2024-09-22',\n",
       " '2024-09-23',\n",
       " '2024-09-24',\n",
       " '2024-09-25',\n",
       " '2024-09-26',\n",
       " '2024-09-27',\n",
       " '2024-09-28',\n",
       " '2024-09-29',\n",
       " '2024-09-30',\n",
       " '2024-10-01',\n",
       " '2024-10-02',\n",
       " '2024-10-03',\n",
       " '2024-10-04',\n",
       " '2024-10-05',\n",
       " '2024-10-06',\n",
       " '2024-10-07',\n",
       " '2024-10-08',\n",
       " '2024-10-09',\n",
       " '2024-10-10',\n",
       " '2024-10-11',\n",
       " '2024-10-12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dates_reprocessing(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "        print(d.strftime('%Y-%m-%d'))\n",
    "    return days_date\n",
    "\n",
    "generate_dates_reprocessing(today, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
