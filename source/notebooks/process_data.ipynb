{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/15 19:43:43 WARN Utils: Your hostname, Ubuntu-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/10/15 19:43:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/15 19:43:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Extraction_Data\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datetime.now().year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = 2023\n",
    "ano_final = 2024\n",
    "for ano in range(int(ano_inicial), int(ano_final) + 1):\n",
    "    print(ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    }
   ],
   "source": [
    "ano_inicial = current_year\n",
    "ano_final = current_year\n",
    "\n",
    "dif_anos = (ano_final - ano_inicial) + 1\n",
    "for i in range(dif_anos):\n",
    "    print(ano_inicial + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- autorEmenda: string (nullable = true)\n",
      " |-- codigoAmparoLegal: long (nullable = true)\n",
      " |-- descricaoEmpenho: string (nullable = true)\n",
      " |-- fonteRecurso: string (nullable = true)\n",
      " |-- idProjetoInvestimento: string (nullable = true)\n",
      " |-- informacoesComplementares: string (nullable = true)\n",
      " |-- localEntrega: string (nullable = true)\n",
      " |-- naturezaDespesa: string (nullable = true)\n",
      " |-- nomeEsferaOrcamentaria: string (nullable = true)\n",
      " |-- nomeFavorecido: string (nullable = true)\n",
      " |-- nomeTipoEmpenho: string (nullable = true)\n",
      " |-- nrPtres: string (nullable = true)\n",
      " |-- numeroNotaEmpenhoGerada: string (nullable = true)\n",
      " |-- numeroProcesso: string (nullable = true)\n",
      " |-- pagina: long (nullable = true)\n",
      " |-- planoInterno: string (nullable = true)\n",
      " |-- planoOrcamentario: string (nullable = true)\n",
      " |-- resultadoPrimario: string (nullable = true)\n",
      " |-- tamanhoDaPagina: long (nullable = true)\n",
      " |-- tipoCredito: string (nullable = true)\n",
      " |-- ugEmitente: string (nullable = true)\n",
      " |-- ugResponsavel: long (nullable = true)\n",
      " |-- unidadeOrcamentaria: string (nullable = true)\n",
      " |-- valorEmpenho: double (nullable = true)\n",
      " |-- nomeArquivo: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/15 19:43:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring\n",
    "\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "process_all = True\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/' + str(ano_inicial)+ '.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "\n",
    "#print(df.show())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m         drop_columns\u001b[38;5;241m.\u001b[39mappend(column\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     29\u001b[0m drop_columns\n\u001b[0;32m---> 31\u001b[0m df_eixos \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m df_eixos\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#df.schema[5].dataType\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#results = spark.sql(\"select idUnico, eixos.eixos.* from eixos\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#results.show()\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/dataframe.py:5439\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   5437\u001b[0m         java_columns\u001b[38;5;241m.\u001b[39mappend(c\u001b[38;5;241m.\u001b[39m_jc)\n\u001b[1;32m   5438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5439\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5440\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5441\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(c)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5442\u001b[0m         )\n\u001b[1;32m   5444\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(java_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, substring, explode, col, to_date\n",
    "from pyspark.sql.types import ArrayType\n",
    "origin = '/home/adriano/Documentos/airflow/database/dest/bronze/projeto_investimento/date'\n",
    "process_all = False\n",
    "ano_final = current_year\n",
    "ano_inicial = current_year\n",
    "\n",
    "\n",
    "if process_all == False:\n",
    "    origin_file = origin + '/2024-09-18.json'\n",
    "    df = spark.read.json(origin_file)\n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-15,10))\n",
    "elif process_all == True:\n",
    "    origin_file = origin + '/*.json'\n",
    "    df = spark.read.json(origin_file) \n",
    "    df = df.withColumn('nomeArquivo',substring(input_file_name(),-9,4))\n",
    "\n",
    "list_drop = ['tomadores','executores', 'teste']\n",
    "\n",
    "df_eixos = df.drop()\n",
    "df_eixos = df_eixos.withColumn(\"dataCadastro\", to_date(col(\"dataCadastro\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "drop_columns = []\n",
    "\n",
    "for column in df.schema:\n",
    "    if type(column.dataType) == ArrayType:\n",
    "        drop_columns.append(column.name)\n",
    "\n",
    "drop_columns\n",
    "\n",
    "df_eixos = df.drop(*drop_columns)\n",
    "df_eixos.printSchema()\n",
    "#df.schema[5].dataType\n",
    "\n",
    "\n",
    "\n",
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')\n",
    "\n",
    "#print(df.show())\n",
    "#print(df.printSchema())\n",
    "#df.createOrReplaceTempView(\"eixos\")\n",
    "\n",
    "#results = spark.sql(\"select idUnico, eixos.eixos.* from eixos\")\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "today = datetime.now()\n",
    "date_before = today - timedelta(days=30)\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "curs = conn.cursor()\n",
    "curs.execute(f'''delete from stg_projeto_investimento_eixos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tomadores     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_executores    where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_repassadores  where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_tipos         where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_sub_tipos     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_geometria     where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento_fontes_de_recurso where \"datacadastro\" between '{date_before}' and '{today}';\n",
    "                 delete from stg_projeto_investimento               where \"datacadastro\" between '{date_before}' and '{today}';''')\n",
    "\n",
    "mode = 'append'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "df_eixos.write.jdbc(url=url, table='stg_projeto_investimento_fontes_de_recurso', mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-09-13T13:31:02.426132'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_before.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "dotenv_path = Path('/home/adriano/Documentos/airflow/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "user_dw = os.getenv('USER_DW')\n",
    "password_dw = os.getenv('PASSWORD_DW')\n",
    "host_dw = os.getenv('HOST_DW')\n",
    "database_dw = os.getenv('DATABASE_DW')\n",
    "port_dw = os.getenv('PORT_DW')\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "mode = 'append'\n",
    "url = f'jdbc:postgresql://{host_dw}:{port_dw}/{database_dw}'\n",
    "properties = {\"user\": user_dw, \"password\": password_dw, \"driver\": driver}\n",
    "#conn = jaydebeapi.connect(driver, url, [user_dw, password_dw], '/home/adriano/Documentos/airflow/jdbc/postgresql-42.7.4.jar')\n",
    "#df.write.jdbc(url=url, table='stg_execucao_financeira', mode=mode, properties=properties)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curs = conn.cursor()\n",
    "#curs.execute(f'''delete from stg_execucao_financeira where \"nomeArquivo\" = '{ano_final}' ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Executions: 1\n",
      "Pages: 0\n",
      "N° Registers: 0\n",
      "Errors: 1\n",
      "Errors Consecutives: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 2\n",
      "Pages: 1\n",
      "N° Registers: 99\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 3\n",
      "Pages: 2\n",
      "N° Registers: 198\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 4\n",
      "Pages: 3\n",
      "N° Registers: 298\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 5\n",
      "Pages: 4\n",
      "N° Registers: 398\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 6\n",
      "Pages: 5\n",
      "N° Registers: 497\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 7\n",
      "Pages: 6\n",
      "N° Registers: 597\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 8\n",
      "Pages: 7\n",
      "N° Registers: 696\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 9\n",
      "Pages: 8\n",
      "N° Registers: 796\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 10\n",
      "Pages: 9\n",
      "N° Registers: 896\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 11\n",
      "Pages: 10\n",
      "N° Registers: 994\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 12\n",
      "Pages: 11\n",
      "N° Registers: 1094\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 13\n",
      "Pages: 12\n",
      "N° Registers: 1193\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 14\n",
      "Pages: 13\n",
      "N° Registers: 1293\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 15\n",
      "Pages: 14\n",
      "N° Registers: 1392\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 16\n",
      "Pages: 15\n",
      "N° Registers: 1492\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 17\n",
      "Pages: 16\n",
      "N° Registers: 1590\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 18\n",
      "Pages: 17\n",
      "N° Registers: 1689\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 19\n",
      "Pages: 18\n",
      "N° Registers: 1780\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 20\n",
      "Pages: 19\n",
      "N° Registers: 1880\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pages: 20\n",
      "N° Registers: 1964\n",
      "Errors: 1\n",
      "Errors Consecutives: 0\n",
      "\n",
      "Execution Finished with success!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def extract_data_api(url_base, endpoint, uf, dest_path, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    method = \"GET\"\n",
    "    dest_path_file = dest_path + '/' + str(uf) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        url = url_base + endpoint + '?' + 'pagina=' + str(page) + '&' + 'tamanhoDaPagina=' + str(page_size) + '&' + 'uf=' + str(uf)\n",
    "        response = requests.request(method, url)\n",
    "        if response.status_code == 200:\n",
    "            if(len(response.json()[\"content\"]) == 0):\n",
    "                success = True \n",
    "            else:\n",
    "                page += 1\n",
    "                errors_consecutives = 0\n",
    "                executions += 1\n",
    "                content_all += response.json()[\"content\"]\n",
    "                #with open(dest_path_file, 'w') as f:\n",
    "                #    json.dump(response.json()[\"content\"],f)\n",
    "        else:\n",
    "            errors_consecutives += 1\n",
    "            errors += 1\n",
    "            executions += 1\n",
    "            if response.status_code == 429:\n",
    "                time.sleep(1)\n",
    "        print(f'Status Code: {response.status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "page_size = 100\n",
    "url_base = \"https://api.obrasgov.gestao.gov.br\"\n",
    "endpoint = \"/obrasgov/api/projeto-investimento\"\n",
    "current_year = datetime.now().year\n",
    "initial_year = int(current_year)\n",
    "final_year = int(current_year)\n",
    "dest_path = '/home/adriano/Documentos/airflow/database/dest/bronze/execucao-financeira'\n",
    "errors_limit = - 1\n",
    "errors_consecutives_limit = 5\n",
    "executions_limit = -1\n",
    "\n",
    "extract_data_api(url_base, endpoint, 'PR', dest_path, page_size, errors_limit, errors_consecutives_limit, executions_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-11 10:22:48.977500\n",
      "2024-10-12 10:22:48.977500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "today = datetime.today()\n",
    "days_before = today - timedelta(days=2)\n",
    "\n",
    "\n",
    "date_generated = [days_before + timedelta(days=x) for x in range(0, (today-days_before).days)]\n",
    "\n",
    "for d in date_generated:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(url_base, endpoint, param, method, path_dest_env, fun_api, page_size, errors_limit, errors_consecutives_limit, executions_limit):\n",
    "    success = False\n",
    "    page = 0\n",
    "    errors_consecutives = 0\n",
    "    errors = 0\n",
    "    executions = 0\n",
    "    dest_path = os.getenv(path_dest_env)\n",
    "    dest_path_file = dest_path + '/' + str(param) + '.json'\n",
    "\n",
    "    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(dest_path_file) == True:\n",
    "        os.remove(dest_path_file)\n",
    "\n",
    "    content_all = []\n",
    "\n",
    "    while success == False and (errors_consecutives < errors_consecutives_limit or errors_consecutives_limit == -1) and (errors < errors_limit or errors_limit == -1) and (executions < executions_limit or executions_limit == -1):\n",
    "        success, content,  page, errors_consecutives, errors, executions, status_code =  fun_api(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions)\n",
    "\n",
    "        if content is not None:\n",
    "            content_all.append(content)\n",
    "\n",
    "        print(f'Status Code: {status_code}\\n'\n",
    "            f'Executions: {executions}\\n'\n",
    "            f'Pages: {page}\\n' \n",
    "            f'N° Registers: {len(content_all)}\\n'\n",
    "            f'Errors: {errors}\\n'\n",
    "            f'Errors Consecutives: {errors_consecutives}\\n')\n",
    "        time.sleep(1)\n",
    "\n",
    "    if success == True:\n",
    "        print('Execution Finished with success!')\n",
    "    else:\n",
    "        print('Execution Finished with error!')\n",
    "        if errors_consecutives < errors_consecutives_limit:\n",
    "            raise Exception(\"Number of consecutives errors exceeded\")\n",
    "        elif errors < errors_limit:\n",
    "            raise Exception(\"Number of total errors exceeded\")\n",
    "        elif executions < executions_limit:\n",
    "            raise Exception(\"Number of executions exceeded\")\n",
    "\n",
    "    with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_all, f)\n",
    "    f.close()\n",
    "\n",
    "def extract_api_projeto_investimento_uf(url_base, endpoint, param, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"uf\": str(param)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        if(len(response.json()[\"content\"]) == 0):\n",
    "            success = True\n",
    "            return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "        else:\n",
    "            page += 1\n",
    "            errors_consecutives = 0\n",
    "            executions += 1\n",
    "            content = response.json()[\"content\"]\n",
    "            return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code\n",
    "\n",
    "@task()\n",
    "def extract_data_api_project(url_base, endpoint, date, page_size = 100, errors_limit = -1, errors_consecutives_limit = 5, executions_limit = 200):\n",
    "    method = \"GET\"\n",
    "    path_dest_env = \"PATH_DEST_PROJETO_INVESTIMENTO\"\n",
    "    extract_api(url_base, endpoint, date, method, path_dest_env, extract_api_projeto_investimento_uf, page_size, errors_limit, errors_consecutives_limit, executions_limit)\n",
    "\n",
    "def extract_api_execucao_financeira_year(url_base, endpoint, year, method, page_size, page, errors_consecutives, errors, executions):\n",
    "    params = {\"pagina\": str(page), \"tamanhoDaPagina\": str(page_size), \"anoFinal\": str(year), \"anoInicial\": str(year)}\n",
    "    url = generate_url(url_base, endpoint, params)\n",
    "    success = False\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        page += 1\n",
    "        errors_consecutives = 0\n",
    "        executions += 1\n",
    "        content = response.json()[\"content\"]\n",
    "        return success, content,  page, errors_consecutives, errors, executions, response.status_code\n",
    "    elif response.status_code == 404:\n",
    "        success = True\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code \n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "        return success, None,  page, errors_consecutives, errors, executions, response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13\n",
      "2024-09-14\n",
      "2024-09-15\n",
      "2024-09-16\n",
      "2024-09-17\n",
      "2024-09-18\n",
      "2024-09-19\n",
      "2024-09-20\n",
      "2024-09-21\n",
      "2024-09-22\n",
      "2024-09-23\n",
      "2024-09-24\n",
      "2024-09-25\n",
      "2024-09-26\n",
      "2024-09-27\n",
      "2024-09-28\n",
      "2024-09-29\n",
      "2024-09-30\n",
      "2024-10-01\n",
      "2024-10-02\n",
      "2024-10-03\n",
      "2024-10-04\n",
      "2024-10-05\n",
      "2024-10-06\n",
      "2024-10-07\n",
      "2024-10-08\n",
      "2024-10-09\n",
      "2024-10-10\n",
      "2024-10-11\n",
      "2024-10-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2024-09-13',\n",
       " '2024-09-14',\n",
       " '2024-09-15',\n",
       " '2024-09-16',\n",
       " '2024-09-17',\n",
       " '2024-09-18',\n",
       " '2024-09-19',\n",
       " '2024-09-20',\n",
       " '2024-09-21',\n",
       " '2024-09-22',\n",
       " '2024-09-23',\n",
       " '2024-09-24',\n",
       " '2024-09-25',\n",
       " '2024-09-26',\n",
       " '2024-09-27',\n",
       " '2024-09-28',\n",
       " '2024-09-29',\n",
       " '2024-09-30',\n",
       " '2024-10-01',\n",
       " '2024-10-02',\n",
       " '2024-10-03',\n",
       " '2024-10-04',\n",
       " '2024-10-05',\n",
       " '2024-10-06',\n",
       " '2024-10-07',\n",
       " '2024-10-08',\n",
       " '2024-10-09',\n",
       " '2024-10-10',\n",
       " '2024-10-11',\n",
       " '2024-10-12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dates_reprocessing(date, days):\n",
    "    days_date = []\n",
    "    date_before = date - timedelta(days=days)\n",
    "    date_generated = [date_before + timedelta(days=x) for x in range(0, (date-date_before).days)]\n",
    "\n",
    "    for d in date_generated:\n",
    "        days_date.append(str(d.strftime('%Y-%m-%d')))\n",
    "        print(d.strftime('%Y-%m-%d'))\n",
    "    return days_date\n",
    "\n",
    "generate_dates_reprocessing(today, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
