{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  ['/home/adriano/Documentos/airflow', '/tmp/spark-9ee79a9f-a12e-4de3-b111-d7da795665e6/userFiles-19658a3e-7ed0-480a-8a85-ce81f1ac5b02', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/home/adriano/Documentos/airflow/airflow/lib/python3.12/site-packages', '/home/adriano/Documentos/airflow/config', '/home/adriano/Documentos/airflow/plugins', '/home/adriano/Documentos/airflow/dags']\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory \" , sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parent = os.path.abspath(os.path.join(os.getcwd(), '..', 'tasks'))\n",
    "sys.path.insert(0, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2025]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Caminho para o diretório raiz do projeto (que contém 'tasks' e 'source')\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "from tasks.operations_task import generate_years\n",
    "\n",
    "generate_years.function(2025,2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "CRUD starting for table public.fact_projeto_investimento\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_FONTE_RECURSO\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_EIXOS\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_TOMADORES\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_EXECUTORES\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_REPASSADORES\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_TIPOS\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_SUB_TIPOS\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tasks.operations_task import crud_facts\n",
    "\n",
    "crud_facts.function(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUD starting for table public.fact_projeto_investimento\n",
      "Starting update...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/26 13:59:26 WARN BlockManager: Putting block rdd_70_0 failed due to exception org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \".\n",
      "25/07/26 13:59:26 WARN BlockManager: Block rdd_70_0 could not be removed as it was not found on disk or in memory\n",
      "25/07/26 13:59:26 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 16)\n",
      "org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \"\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/07/26 13:59:26 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 16) (10.0.2.15 executor driver): org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \"\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n",
      "\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/07/26 13:59:26 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o294.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16) (10.0.2.15 executor driver): org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \"\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \"\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcrud_facts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/airflow/tasks/operations_task.py:219\u001b[0m, in \u001b[0;36mcrud_facts\u001b[0;34m(bulk_size)\u001b[0m\n\u001b[1;32m    217\u001b[0m table_name_project_inv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic.fact_projeto_investimento\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m key_columns_project_inv  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk_projeto_investimento\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m \u001b[43mcrud_database_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_old_project_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_new_project_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name_project_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_columns_project_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# CRUD fact_projeto_investimento_fonte_recurso\u001b[39;00m\n\u001b[1;32m    222\u001b[0m sql_new_project_inv_font_rec \u001b[38;5;241m=\u001b[39m     \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124m                                            DPI.SK_PROJETO_INVESTIMENTO,\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124m                                            DFR.SK_FONTE_RECURSO,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m                                    INNER JOIN STG_PROJETO_INVESTIMENTO_FONTES_DE_RECURSO FR ON FR.IDUNICO = STG.IDUNICO \u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m                                    INNER JOIN DIM_FONTE_RECURSO            DFR ON DFR.NK_FONTE_RECURSO = UPPER(FR.ORIGEM)\u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "File \u001b[0;32m~/Documentos/airflow/scripts/functions_util.py:320\u001b[0m, in \u001b[0;36mcrud_database_table\u001b[0;34m(spark, sql_old, sql_new, table_name, key_columns, connection_properties, bulk_size, insert, update, delete)\u001b[0m\n\u001b[1;32m    304\u001b[0m df_new \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)\\\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, sql_new)\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, connection_properties[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m])\\\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    312\u001b[0m df_old \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)\\\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, sql_old)\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, connection_properties[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m])\\\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m--> 320\u001b[0m \u001b[43mcrud_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minsert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m df_new\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m    323\u001b[0m df_old\u001b[38;5;241m.\u001b[39munpersist()\n",
      "File \u001b[0;32m~/Documentos/airflow/scripts/functions_util.py:289\u001b[0m, in \u001b[0;36mcrud_database\u001b[0;34m(df_old, df_new, table_name, key_columns, connection_properties, bulk_size, insert, update, delete)\u001b[0m\n\u001b[1;32m    287\u001b[0m df_update \u001b[38;5;241m=\u001b[39m values_to_update(df_old, df_new, key_columns, columns_comparisson)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting update...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 289\u001b[0m \u001b[43mbulk_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUPDATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns_comparisson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_update\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    291\u001b[0m df_update\u001b[38;5;241m.\u001b[39munpersist()\n",
      "File \u001b[0;32m~/Documentos/airflow/scripts/functions_util.py:238\u001b[0m, in \u001b[0;36mbulk_values\u001b[0;34m(table_name, df, bulk_size, connection_properties, type, columns, key_columns)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbulk_values\u001b[39m(table_name, df, bulk_size, connection_properties, \u001b[38;5;28mtype\u001b[39m, columns, key_columns\u001b[38;5;241m=\u001b[39m[]):\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     splits \u001b[38;5;241m=\u001b[39m ceil(df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m/\u001b[39m bulk_size)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o294.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16) (10.0.2.15 executor driver): org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \"\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.postgresql.util.PSQLException: ERROR: invalid input syntax for type integer: \" \"\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:137)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:275)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-25T20:31:43.188-0300\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from /home/adriano/Documentos/airflow/dags\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:43.812-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4435} INFO\u001b[0m - dagrun id: dag_teste\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:43.825-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4451} INFO\u001b[0m - created dagrun <DagRun dag_teste @ 2025-07-25 23:31:43.791457+00:00: manual__2025-07-25T23:31:43.791457+00:00, state:running, queued_at: None. externally triggered: False>\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:43.885-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4396} INFO\u001b[0m - [DAG TEST] starting task_id=crud_projeto_investimento.delete_duplicates_stg map_index=-1\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:43.886-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4399} INFO\u001b[0m - [DAG TEST] running task <TaskInstance: dag_teste.crud_projeto_investimento.delete_duplicates_stg manual__2025-07-25T23:31:43.791457+00:00 [scheduled]>\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:43.930-0300\u001b[0m] {\u001b[34mworkday.py:\u001b[0m41} \u001b[33mWARNING\u001b[0m - \u001b[33mCould not import pandas. Holidays will not be considered.\u001b[0m\n",
      "[2025-07-25 20:31:44,007] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag_teste' AIRFLOW_CTX_TASK_ID='crud_projeto_investimento.delete_duplicates_stg' AIRFLOW_CTX_EXECUTION_DATE='2025-07-25T23:31:43.791457+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-25T23:31:43.791457+00:00'\n",
      "[\u001b[34m2025-07-25T20:31:44.007-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m3131} INFO\u001b[0m - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag_teste' AIRFLOW_CTX_TASK_ID='crud_projeto_investimento.delete_duplicates_stg' AIRFLOW_CTX_EXECUTION_DATE='2025-07-25T23:31:43.791457+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-25T23:31:43.791457+00:00'\u001b[0m\n",
      "Task instance is in running state\n",
      " Previous state of the Task instance: queued\n",
      "Current task name:crud_projeto_investimento.delete_duplicates_stg state:scheduled start_date:None\n",
      "Dag name:dag_teste and current dag run status:running\n",
      "[\u001b[34m2025-07-25T20:31:44.018-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m731} INFO\u001b[0m - ::endgroup::\u001b[0m\n",
      "Deleting duplicates on table stg_projeto_investimento_eixos...\n",
      "Deleting duplicates on table stg_projeto_investimento_executores...\n",
      "Deleting duplicates on table stg_projeto_investimento_fontes_de_recurso...\n",
      "Deleting duplicates on table stg_projeto_investimento_repassadores...\n",
      "Deleting duplicates on table stg_projeto_investimento_sub_tipos...\n",
      "Deleting duplicates on table stg_projeto_investimento_tipos...\n",
      "Deleting duplicates on table stg_projeto_investimento_tomadores...\n",
      "Deleting duplicates on table stg_projeto_investimento...\n",
      "Deleting duplicates on table stg_execucao_financeira using distinct...\n",
      "[2025-07-25 20:31:44,519] {python.py:240} INFO - Done. Returned value was: None\n",
      "[\u001b[34m2025-07-25T20:31:44.519-0300\u001b[0m] {\u001b[34mpython.py:\u001b[0m240} INFO\u001b[0m - Done. Returned value was: None\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:44.543-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m340} INFO\u001b[0m - ::group::Post task execution logs\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:44.544-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m352} INFO\u001b[0m - Marking task as SUCCESS. dag_id=dag_teste, task_id=crud_projeto_investimento.delete_duplicates_stg, run_id=manual__2025-07-25T23:31:43.791457+00:00, execution_date=20250725T233143, start_date=, end_date=20250725T233144\u001b[0m\n",
      "Task instance in success state\n",
      " Previous state of the Task instance: running\n",
      "Dag name:dag_teste queued_at:None\n",
      "Task hostname:Ubuntu-VirtualBox operator:_PythonDecoratedOperator\n",
      "[\u001b[34m2025-07-25T20:31:44.554-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4410} INFO\u001b[0m - [DAG TEST] end task task_id=crud_projeto_investimento.delete_duplicates_stg map_index=-1\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:44.574-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4396} INFO\u001b[0m - [DAG TEST] starting task_id=crud_projeto_investimento.crud_dimensions map_index=-1\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:31:44.575-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4399} INFO\u001b[0m - [DAG TEST] running task <TaskInstance: dag_teste.crud_projeto_investimento.crud_dimensions manual__2025-07-25T23:31:43.791457+00:00 [scheduled]>\u001b[0m\n",
      "[2025-07-25 20:31:44,639] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag_teste' AIRFLOW_CTX_TASK_ID='crud_projeto_investimento.crud_dimensions' AIRFLOW_CTX_EXECUTION_DATE='2025-07-25T23:31:43.791457+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-25T23:31:43.791457+00:00'\n",
      "[\u001b[34m2025-07-25T20:31:44.639-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m3131} INFO\u001b[0m - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag_teste' AIRFLOW_CTX_TASK_ID='crud_projeto_investimento.crud_dimensions' AIRFLOW_CTX_EXECUTION_DATE='2025-07-25T23:31:43.791457+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-25T23:31:43.791457+00:00'\u001b[0m\n",
      "Task instance is in running state\n",
      " Previous state of the Task instance: queued\n",
      "Current task name:crud_projeto_investimento.crud_dimensions state:scheduled start_date:None\n",
      "Dag name:dag_teste and current dag run status:running\n",
      "[\u001b[34m2025-07-25T20:31:44.641-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m731} INFO\u001b[0m - ::endgroup::\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/25 20:31:48 WARN Utils: Your hostname, Ubuntu-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/07/25 20:31:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/25 20:31:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUD starting for table public.dim_eixo\n",
      "Starting update...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/25 20:32:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 0 rows\n",
      "Starting insert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_executor\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_fonte_recurso\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_repassador\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_sub_tipo\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_tipo\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_tomador\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_uf\n",
      "Starting update...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_situacao\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_natureza\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_especie\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.dim_projeto_investimento\n",
      "Starting update...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "[2025-07-25 20:32:48,089] {python.py:240} INFO - Done. Returned value was: None\n",
      "[\u001b[34m2025-07-25T20:32:48.089-0300\u001b[0m] {\u001b[34mpython.py:\u001b[0m240} INFO\u001b[0m - Done. Returned value was: None\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:32:48.111-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m340} INFO\u001b[0m - ::group::Post task execution logs\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:32:48.120-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m352} INFO\u001b[0m - Marking task as SUCCESS. dag_id=dag_teste, task_id=crud_projeto_investimento.crud_dimensions, run_id=manual__2025-07-25T23:31:43.791457+00:00, execution_date=20250725T233143, start_date=20250725T233146, end_date=20250725T233248\u001b[0m\n",
      "Task instance in success state\n",
      " Previous state of the Task instance: running\n",
      "Dag name:dag_teste queued_at:None\n",
      "Task hostname:Ubuntu-VirtualBox operator:_PythonDecoratedOperator\n",
      "[\u001b[34m2025-07-25T20:32:48.143-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4410} INFO\u001b[0m - [DAG TEST] end task task_id=crud_projeto_investimento.crud_dimensions map_index=-1\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:32:48.177-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4396} INFO\u001b[0m - [DAG TEST] starting task_id=crud_projeto_investimento.crud_facts map_index=-1\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:32:48.180-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4399} INFO\u001b[0m - [DAG TEST] running task <TaskInstance: dag_teste.crud_projeto_investimento.crud_facts manual__2025-07-25T23:31:43.791457+00:00 [scheduled]>\u001b[0m\n",
      "[2025-07-25 20:32:48,251] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag_teste' AIRFLOW_CTX_TASK_ID='crud_projeto_investimento.crud_facts' AIRFLOW_CTX_EXECUTION_DATE='2025-07-25T23:31:43.791457+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-25T23:31:43.791457+00:00'\n",
      "[\u001b[34m2025-07-25T20:32:48.251-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m3131} INFO\u001b[0m - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag_teste' AIRFLOW_CTX_TASK_ID='crud_projeto_investimento.crud_facts' AIRFLOW_CTX_EXECUTION_DATE='2025-07-25T23:31:43.791457+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-25T23:31:43.791457+00:00'\u001b[0m\n",
      "Task instance is in running state\n",
      " Previous state of the Task instance: queued\n",
      "Current task name:crud_projeto_investimento.crud_facts state:scheduled start_date:None\n",
      "Dag name:dag_teste and current dag run status:running\n",
      "[\u001b[34m2025-07-25T20:32:48.282-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m731} INFO\u001b[0m - ::endgroup::\u001b[0m\n",
      "CRUD starting for table public.fact_projeto_investimento\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_FONTE_RECURSO\n",
      "Starting update...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_EIXOS\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_TOMADORES\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_EXECUTORES\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_REPASSADORES\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_TIPOS\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "CRUD starting for table public.FACT_PROJETO_INVESTIMENTO_SUB_TIPOS\n",
      "Starting update...\n",
      "Updated 0 rows\n",
      "Starting insert...\n",
      "Inserted 0 rows\n",
      "[2025-07-25 20:33:13,050] {python.py:240} INFO - Done. Returned value was: None\n",
      "[\u001b[34m2025-07-25T20:33:13.050-0300\u001b[0m] {\u001b[34mpython.py:\u001b[0m240} INFO\u001b[0m - Done. Returned value was: None\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:33:13.060-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m340} INFO\u001b[0m - ::group::Post task execution logs\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:33:13.064-0300\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m352} INFO\u001b[0m - Marking task as SUCCESS. dag_id=dag_teste, task_id=crud_projeto_investimento.crud_facts, run_id=manual__2025-07-25T23:31:43.791457+00:00, execution_date=20250725T233143, start_date=20250725T233250, end_date=20250725T233313\u001b[0m\n",
      "Task instance in success state\n",
      " Previous state of the Task instance: running\n",
      "Dag name:dag_teste queued_at:None\n",
      "Task hostname:Ubuntu-VirtualBox operator:_PythonDecoratedOperator\n",
      "[\u001b[34m2025-07-25T20:33:13.083-0300\u001b[0m] {\u001b[34mdag.py:\u001b[0m4410} INFO\u001b[0m - [DAG TEST] end task task_id=crud_projeto_investimento.crud_facts map_index=-1\u001b[0m\n",
      "[\u001b[34m2025-07-25T20:33:13.086-0300\u001b[0m] {\u001b[34mdagrun.py:\u001b[0m854} INFO\u001b[0m - Marking run <DagRun dag_teste @ 2025-07-25 23:31:43.791457+00:00: manual__2025-07-25T23:31:43.791457+00:00, state:running, queued_at: None. externally triggered: False> successful\u001b[0m\n",
      "Dag run in success state\n",
      "Dag run start:2025-07-25 23:31:43.791457+00:00 end:2025-07-25 23:33:13.087175+00:00\n",
      "[\u001b[34m2025-07-25T20:33:13.087-0300\u001b[0m] {\u001b[34mdagrun.py:\u001b[0m905} INFO\u001b[0m - DagRun Finished: dag_id=dag_teste, execution_date=2025-07-25 23:31:43.791457+00:00, run_id=manual__2025-07-25T23:31:43.791457+00:00, run_start_date=2025-07-25 23:31:43.791457+00:00, run_end_date=2025-07-25 23:33:13.087175+00:00, run_duration=89.295718, state=success, external_trigger=False, run_type=manual, data_interval_start=2025-07-25 23:31:43.791457+00:00, data_interval_end=2025-07-25 23:31:43.791457+00:00, dag_hash=None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from airflow.models import DagBag\n",
    "\n",
    "dag = DagBag(include_examples=False).get_dag(\"dag_teste\")\n",
    "results = dag.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-19T12:10:18.508-0300\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from /home/adriano/Documentos/airflow/dags\u001b[0m\n",
      "[<Task(_PythonDecoratedOperator): extract_execucao_financeira.generate_years>, <Mapped(_PythonDecoratedOperator): extract_execucao_financeira.extract_data_api>, <Mapped(_PythonDecoratedOperator): extract_execucao_financeira.extract_data_json>, <Task(_PythonDecoratedOperator): extract_projeto_investimento.generate_dates>, <Mapped(_PythonDecoratedOperator): extract_projeto_investimento.extract_data_api_projecto_investimento_date>, <Task(_PythonDecoratedOperator): extract_projeto_investimento.delete_stg_projeto_investimento>, <Task(_PythonDecoratedOperator): extract_projeto_investimento.extract_json_projeto_investimento_date>, <Task(_PythonDecoratedOperator): crud_projeto_investimento.crud_dimensions>, <Task(_PythonDecoratedOperator): crud_projeto_investimento.delete_duplicates_stg>, <Task(_PythonDecoratedOperator): crud_projeto_investimento.crud_facts>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TaskInstance.run() got an unexpected keyword argument 'do_xcom_push'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m ti \u001b[38;5;241m=\u001b[39m TaskInstance(task\u001b[38;5;241m=\u001b[39mtask, run_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexec_date\u001b[38;5;241m.\u001b[39misoformat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, execution_date\u001b[38;5;241m=\u001b[39mexec_date)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Executa a task isoladamente\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mti\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_ti_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_xcom_push\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Puxa o valor salvo no XCom\u001b[39;00m\n\u001b[1;32m     21\u001b[0m value \u001b[38;5;241m=\u001b[39m ti\u001b[38;5;241m.\u001b[39mxcom_pull(task_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate_years\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/airflow/airflow/lib/python3.12/site-packages/airflow/utils/session.py:97\u001b[0m, in \u001b[0;36mprovide_session.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TaskInstance.run() got an unexpected keyword argument 'do_xcom_push'"
     ]
    }
   ],
   "source": [
    "from airflow import settings\n",
    "from airflow.models import DagBag, TaskInstance\n",
    "from airflow.utils import timezone\n",
    "\n",
    "# Carrega DAG do projeto\n",
    "dag = DagBag(dag_folder=f\"{os.path.abspath(os.path.join(os.getcwd(), '..', '..'))}/dags\", include_examples=False).get_dag(\"get_api_data\")\n",
    "\n",
    "print(dag.tasks)\n",
    "\n",
    "# Obtém a task\n",
    "task = dag.get_task(\"extract_execucao_financeira.generate_years\").\n",
    "\n",
    "# Cria um TaskInstance\n",
    "exec_date = timezone.utcnow()\n",
    "ti = TaskInstance(task=task, run_id=f\"manual__{exec_date.isoformat()}\", execution_date=exec_date,)\n",
    "\n",
    "# Executa a task isoladamente\n",
    "ti.run(ignore_ti_state=True, do_xcom_push=True,)\n",
    "\n",
    "# Puxa o valor salvo no XCom\n",
    "value = ti.xcom_pull(task_ids=\"generate_years\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 20:45:14 WARN Utils: Your hostname, Ubuntu-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/10/04 20:45:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/04 20:45:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Extraction_Data\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = '../../database/origin/ecomerce_dataset.csv'\n",
    "df = spark.read.option(\"header\", True).csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 20:45:28 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path_write = '../../database/dest/bronze/ecommerce_data'\n",
    "df.write.mode(\"overwrite\").save(path_write, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n"
     ]
    }
   ],
   "source": [
    "current_year = datetime.now().year\n",
    "print(current_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(url_base, endpoint, parameters):\n",
    "    url = url_base + endpoint\n",
    "    i = 0\n",
    "    number_parameters = len(parameters)\n",
    "    if number_parameters > 0:\n",
    "        url += '?'\n",
    "    for parameter in parameters.items():\n",
    "        url += parameter[0] + '=' + str(parameter[1])\n",
    "        i += 1\n",
    "        if i != number_parameters:\n",
    "            url += '&'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.obrasgov.gestao.gov.br/obrasgov/api/execucao-financeira?pagina=0&tamanhoDaPagina=100&anoFinal=2024&anoInicial=2024\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "pagina = 0\n",
    "tamanho_pagina = 100\n",
    "url_base = \"https://api.obrasgov.gestao.gov.br\"\n",
    "endpoint = \"/obrasgov/api/execucao-financeira\"\n",
    "ano_final = str(current_year)\n",
    "ano_inicial = str(current_year)\n",
    "\n",
    "parameters = {\n",
    "    \"tamanhoDaPagina\": tamanho_pagina,\n",
    "    \"pagina\": pagina,\n",
    "    \"anoFinal\": ano_final,\n",
    "    \"anoInicial\": ano_inicial\n",
    "}\n",
    "\n",
    "\n",
    "url = url_base + endpoint + '?' + 'pagina=' + str(pagina) + '&' + 'tamanhoDaPagina=' + str(tamanho_pagina) + '&' + 'anoFinal=' + str(current_year) + '&' + 'anoInicial=' + str(current_year) \n",
    "\n",
    "print(url)\n",
    "\n",
    "response = requests.request(\"GET\", url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.obrasgov.gestao.gov.br/obrasgov/api/execucao-financeira?tamanhoDaPagina=100&pagina=0&anoFinal=2024&anoInicial=2024'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"tamanhoDaPagina\": tamanho_pagina,\n",
    "    \"pagina\": pagina,\n",
    "    \"anoFinal\": ano_final,\n",
    "    \"anoInicial\": ano_inicial\n",
    "}\n",
    "\n",
    "generate_url(url_base,endpoint,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Executions: 1\n",
      "Pagina: 1\n",
      "N° De registros: 138\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 2\n",
      "Pagina: 2\n",
      "N° De registros: 272\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 3\n",
      "Pagina: 3\n",
      "N° De registros: 410\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 4\n",
      "Pagina: 4\n",
      "N° De registros: 540\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 5\n",
      "Pagina: 5\n",
      "N° De registros: 673\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 6\n",
      "Pagina: 6\n",
      "N° De registros: 802\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 7\n",
      "Pagina: 7\n",
      "N° De registros: 928\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 8\n",
      "Pagina: 8\n",
      "N° De registros: 1040\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 9\n",
      "Pagina: 9\n",
      "N° De registros: 1142\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 10\n",
      "Pagina: 10\n",
      "N° De registros: 1261\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 11\n",
      "Pagina: 11\n",
      "N° De registros: 1290\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 12\n",
      "Pagina: 12\n",
      "N° De registros: 1313\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 13\n",
      "Pagina: 13\n",
      "N° De registros: 1360\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 14\n",
      "Pagina: 14\n",
      "N° De registros: 1397\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 15\n",
      "Pagina: 15\n",
      "N° De registros: 1410\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 16\n",
      "Pagina: 16\n",
      "N° De registros: 1438\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 17\n",
      "Pagina: 17\n",
      "N° De registros: 1474\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 18\n",
      "Pagina: 18\n",
      "N° De registros: 1487\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 19\n",
      "Pagina: 19\n",
      "N° De registros: 1503\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 20\n",
      "Pagina: 20\n",
      "N° De registros: 1527\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 21\n",
      "Pagina: 21\n",
      "N° De registros: 1544\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 22\n",
      "Pagina: 22\n",
      "N° De registros: 1566\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 23\n",
      "Pagina: 23\n",
      "N° De registros: 1590\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 24\n",
      "Pagina: 24\n",
      "N° De registros: 1609\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 25\n",
      "Pagina: 25\n",
      "N° De registros: 1637\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 26\n",
      "Pagina: 26\n",
      "N° De registros: 1659\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 27\n",
      "Pagina: 27\n",
      "N° De registros: 1681\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 28\n",
      "Pagina: 28\n",
      "N° De registros: 1708\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 29\n",
      "Pagina: 29\n",
      "N° De registros: 1728\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 30\n",
      "Pagina: 30\n",
      "N° De registros: 1756\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 31\n",
      "Pagina: 31\n",
      "N° De registros: 1784\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 32\n",
      "Pagina: 32\n",
      "N° De registros: 1806\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 33\n",
      "Pagina: 33\n",
      "N° De registros: 1826\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 34\n",
      "Pagina: 34\n",
      "N° De registros: 1848\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 35\n",
      "Pagina: 35\n",
      "N° De registros: 1871\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 36\n",
      "Pagina: 36\n",
      "N° De registros: 1898\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 37\n",
      "Pagina: 37\n",
      "N° De registros: 1927\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 38\n",
      "Pagina: 38\n",
      "N° De registros: 1949\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 39\n",
      "Pagina: 39\n",
      "N° De registros: 1971\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 40\n",
      "Pagina: 40\n",
      "N° De registros: 1993\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 41\n",
      "Pagina: 41\n",
      "N° De registros: 2028\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 42\n",
      "Pagina: 42\n",
      "N° De registros: 2056\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 43\n",
      "Pagina: 43\n",
      "N° De registros: 2095\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 44\n",
      "Pagina: 44\n",
      "N° De registros: 2156\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 45\n",
      "Pagina: 45\n",
      "N° De registros: 2206\n",
      "Erros: 0\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 46\n",
      "Pagina: 45\n",
      "N° De registros: 2206\n",
      "Erros: 1\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 47\n",
      "Pagina: 46\n",
      "N° De registros: 2257\n",
      "Erros: 1\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 48\n",
      "Pagina: 46\n",
      "N° De registros: 2257\n",
      "Erros: 2\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 49\n",
      "Pagina: 47\n",
      "N° De registros: 2315\n",
      "Erros: 2\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 50\n",
      "Pagina: 47\n",
      "N° De registros: 2315\n",
      "Erros: 3\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 51\n",
      "Pagina: 48\n",
      "N° De registros: 2344\n",
      "Erros: 3\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 52\n",
      "Pagina: 49\n",
      "N° De registros: 2393\n",
      "Erros: 3\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 53\n",
      "Pagina: 49\n",
      "N° De registros: 2393\n",
      "Erros: 4\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 54\n",
      "Pagina: 50\n",
      "N° De registros: 2462\n",
      "Erros: 4\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 55\n",
      "Pagina: 50\n",
      "N° De registros: 2462\n",
      "Erros: 5\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 56\n",
      "Pagina: 51\n",
      "N° De registros: 2509\n",
      "Erros: 5\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 57\n",
      "Pagina: 51\n",
      "N° De registros: 2509\n",
      "Erros: 6\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 58\n",
      "Pagina: 52\n",
      "N° De registros: 2573\n",
      "Erros: 6\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 59\n",
      "Pagina: 52\n",
      "N° De registros: 2573\n",
      "Erros: 7\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 60\n",
      "Pagina: 53\n",
      "N° De registros: 2623\n",
      "Erros: 7\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 61\n",
      "Pagina: 54\n",
      "N° De registros: 2675\n",
      "Erros: 7\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 62\n",
      "Pagina: 54\n",
      "N° De registros: 2675\n",
      "Erros: 8\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 63\n",
      "Pagina: 55\n",
      "N° De registros: 2717\n",
      "Erros: 8\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 64\n",
      "Pagina: 55\n",
      "N° De registros: 2717\n",
      "Erros: 9\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 65\n",
      "Pagina: 56\n",
      "N° De registros: 2739\n",
      "Erros: 9\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 66\n",
      "Pagina: 56\n",
      "N° De registros: 2739\n",
      "Erros: 10\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 67\n",
      "Pagina: 57\n",
      "N° De registros: 2755\n",
      "Erros: 10\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 68\n",
      "Pagina: 57\n",
      "N° De registros: 2755\n",
      "Erros: 11\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 69\n",
      "Pagina: 58\n",
      "N° De registros: 2776\n",
      "Erros: 11\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 70\n",
      "Pagina: 59\n",
      "N° De registros: 2794\n",
      "Erros: 11\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 71\n",
      "Pagina: 59\n",
      "N° De registros: 2794\n",
      "Erros: 12\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 72\n",
      "Pagina: 60\n",
      "N° De registros: 2814\n",
      "Erros: 12\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 73\n",
      "Pagina: 60\n",
      "N° De registros: 2814\n",
      "Erros: 13\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 74\n",
      "Pagina: 61\n",
      "N° De registros: 2831\n",
      "Erros: 13\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 75\n",
      "Pagina: 61\n",
      "N° De registros: 2831\n",
      "Erros: 14\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 76\n",
      "Pagina: 62\n",
      "N° De registros: 2850\n",
      "Erros: 14\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 77\n",
      "Pagina: 62\n",
      "N° De registros: 2850\n",
      "Erros: 15\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 78\n",
      "Pagina: 63\n",
      "N° De registros: 2872\n",
      "Erros: 15\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 79\n",
      "Pagina: 64\n",
      "N° De registros: 2893\n",
      "Erros: 15\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 80\n",
      "Pagina: 64\n",
      "N° De registros: 2893\n",
      "Erros: 16\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 81\n",
      "Pagina: 65\n",
      "N° De registros: 2914\n",
      "Erros: 16\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 82\n",
      "Pagina: 65\n",
      "N° De registros: 2914\n",
      "Erros: 17\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 83\n",
      "Pagina: 66\n",
      "N° De registros: 2939\n",
      "Erros: 17\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 84\n",
      "Pagina: 66\n",
      "N° De registros: 2939\n",
      "Erros: 18\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 85\n",
      "Pagina: 67\n",
      "N° De registros: 2956\n",
      "Erros: 18\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 86\n",
      "Pagina: 67\n",
      "N° De registros: 2956\n",
      "Erros: 19\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 87\n",
      "Pagina: 68\n",
      "N° De registros: 2983\n",
      "Erros: 19\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 88\n",
      "Pagina: 69\n",
      "N° De registros: 3010\n",
      "Erros: 19\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 89\n",
      "Pagina: 69\n",
      "N° De registros: 3010\n",
      "Erros: 20\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 90\n",
      "Pagina: 70\n",
      "N° De registros: 3053\n",
      "Erros: 20\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 91\n",
      "Pagina: 70\n",
      "N° De registros: 3053\n",
      "Erros: 21\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 92\n",
      "Pagina: 71\n",
      "N° De registros: 3091\n",
      "Erros: 21\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 93\n",
      "Pagina: 71\n",
      "N° De registros: 3091\n",
      "Erros: 22\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 94\n",
      "Pagina: 72\n",
      "N° De registros: 3129\n",
      "Erros: 22\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 95\n",
      "Pagina: 72\n",
      "N° De registros: 3129\n",
      "Erros: 23\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 96\n",
      "Pagina: 73\n",
      "N° De registros: 3173\n",
      "Erros: 23\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 97\n",
      "Pagina: 74\n",
      "N° De registros: 3211\n",
      "Erros: 23\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 98\n",
      "Pagina: 74\n",
      "N° De registros: 3211\n",
      "Erros: 24\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 99\n",
      "Pagina: 75\n",
      "N° De registros: 3251\n",
      "Erros: 24\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 100\n",
      "Pagina: 75\n",
      "N° De registros: 3251\n",
      "Erros: 25\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 101\n",
      "Pagina: 76\n",
      "N° De registros: 3264\n",
      "Erros: 25\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 102\n",
      "Pagina: 76\n",
      "N° De registros: 3264\n",
      "Erros: 26\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 103\n",
      "Pagina: 77\n",
      "N° De registros: 3267\n",
      "Erros: 26\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 104\n",
      "Pagina: 77\n",
      "N° De registros: 3267\n",
      "Erros: 27\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 105\n",
      "Pagina: 78\n",
      "N° De registros: 3271\n",
      "Erros: 27\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 106\n",
      "Pagina: 79\n",
      "N° De registros: 3291\n",
      "Erros: 27\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 107\n",
      "Pagina: 79\n",
      "N° De registros: 3291\n",
      "Erros: 28\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 200\n",
      "Executions: 108\n",
      "Pagina: 80\n",
      "N° De registros: 3292\n",
      "Erros: 28\n",
      "Erros Consecutivos: 0\n",
      "\n",
      "Status Code: 429\n",
      "Executions: 109\n",
      "Pagina: 80\n",
      "N° De registros: 3292\n",
      "Erros: 29\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Status Code: 404\n",
      "Executions: 109\n",
      "Pagina: 80\n",
      "N° De registros: 3292\n",
      "Erros: 29\n",
      "Erros Consecutivos: 1\n",
      "\n",
      "Execução Finalizada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "dest_path = '../../database/dest/bronze/execucao-financeira'\n",
    "success = False\n",
    "pagina = 0\n",
    "errors_consecutives = 0\n",
    "errors_consecutives_limit = 5\n",
    "errors = 0\n",
    "errors_limit = 50\n",
    "executions = 0\n",
    "executions_limit = 200\n",
    "method = \"GET\"\n",
    "\n",
    "shutil.rmtree(dest_path)\n",
    "\n",
    "Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "content_all = []\n",
    "\n",
    "while success == False and errors_consecutives < errors_consecutives_limit and errors < errors_limit and executions < executions_limit:\n",
    "    response = requests.request(method, url)\n",
    "    if response.status_code == 200:\n",
    "        dest_path_file = dest_path + '/' + str(current_year) + '_' + str(pagina) + '.json'\n",
    "        pagina += 1\n",
    "        errors_consecutives = 0\n",
    "        executions += 1\n",
    "        content_all += response.json()[\"content\"]\n",
    "        #with open(dest_path_file, 'w') as f:\n",
    "        #    json.dump(response.json()[\"content\"],f)\n",
    "    elif response.status_code == 404:\n",
    "        success = True \n",
    "    else:\n",
    "        errors_consecutives += 1\n",
    "        errors += 1\n",
    "        executions += 1\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(1)\n",
    "    url = url_base + endpoint + '?' + 'pagina=' + str(pagina) + '&' + 'tamanhoDaPagina=' + str(tamanho_pagina) + '&' + 'anoFinal=' + str(current_year) + '&' + 'anoInicial=' + str(current_year) \n",
    "    print(f'Status Code: {response.status_code}\\n'\n",
    "           f'Executions: {executions}\\n'\n",
    "           f'Pagina: {pagina}\\n' \n",
    "           f'N° De registros: {len(content_all)}\\n'\n",
    "           f'Erros: {errors}\\n'\n",
    "           f'Erros Consecutivos: {errors_consecutives}\\n')\n",
    "    time.sleep(1)\n",
    "\n",
    "if success == True:\n",
    "    print('Execução Finalizada com sucesso!')\n",
    "else:\n",
    "    print('Execução Finalizada com falha!')\n",
    "\n",
    "dest_path_file = dest_path + '/' + str(ano_inicial) + '_' + str(ano_final) + '.json'\n",
    "\n",
    "with open(dest_path_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(content_all, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../database/dest/bronze/execucao-financeira/2024_2024.json'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#response.json()\n",
    "dest_path_file = dest_path + '/' + str(ano_inicial) + '_' + str(ano_final) + '.json'\n",
    "dest_path_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
